{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37b343b",
   "metadata": {},
   "source": [
    "roberta (RoBERTa): Thực chất là mô hình BERT với một số tinh chỉnh để tạo ra được model này. Các điểm khác biệt ở RoBERTa so với BERT là:\n",
    "*  RoBERTa được đào tạo trên tập dữ liệu lớn hơn rất nhiều so với BERT (160GB dữ liệu).\n",
    "*  RoBERTa sử dynamic mask thay vì sử dụng static mask so với BERT. Ở BERT sử dụng một mask và dự đoán mask một cách ngẫu nhiên trong quá trình tiền xử lý, dẫn đến chỉ sử dụng duy nhất một static mask. Còn ở RoBERTa thì mask được thực hiện trong quá trình đào tạo nên từ đó số lượng mask sẽ không bị giới hạn như BERT cho kết quả tốt hơn một chút so với stactic mask\n",
    "*  RoBERTa được huấn luyện các câu dài hơn BERT và huấn luyện trên batch lớn hơn.\n",
    "*  Về Tokenization RoBERTa sử dụng byte-level Byte-Pair Encoding( BPE) với kích thước từ vựng là 50.000.\n",
    "*  RoBERTa không sử NSP trong quá trình huấn luyện.\n",
    "\n",
    "RoBERTa thích hợp sử dụng cho việc phân loại hơn là việc tạo ra văn bản."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
