{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Report.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "483e6c5d",
        "hXPOK1RUX9le",
        "YZRCzUMeWKdK",
        "bZ36yrMAvLjL",
        "ZD9PyELAAVBL",
        "cQlF4Y-G-1eT",
        "lx77E83MBvzn",
        "98GQ3mdcY7ii",
        "bj6vm-xJEEpj"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b2dcf7c"
      },
      "source": [
        "# B√°o c√°o ƒë·ªì √°n m√¥n \"Khoa h·ªçc d·ªØ li·ªáu ·ª©ng d·ª•ng\"\n",
        "\n",
        "Nh√≥m 4:\n",
        "1. 1712615 - Nguy·ªÖn Tr·ªçng Nghƒ©a - [GitHub](https://github.com/nguyentrongnghia142)\n",
        "2. 1712683 - Ph·∫°m Ho√†ng Ph∆∞∆°ng - [Github](https://github.com/superman19993)\n",
        "3. 1712718 - Hu·ª≥nh Thanh Sang - [GitHub](https://github.com/hts7117)\n",
        "4. 1712584 - Nguy·ªÖn C√¥ng L√Ω - [GitHub](https://github.com/conglyne222)\n",
        "\n",
        "Link th√πng ch·ª©a Github c·ªßa nh√≥m: [Repo](https://github.com/nguyentrongnghia142/CommonLit-Readability-Prize)"
      ],
      "id": "5b2dcf7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f93d974"
      },
      "source": [
        "## M√¥ t·∫£ b√†i to√°n\n",
        "[B√†i to√°n](https://www.kaggle.com/c/commonlitreadabilityprize/overview)"
      ],
      "id": "2f93d974"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTgFA0XYrcWo"
      },
      "source": [
        "### B·ªëi c·∫£nh\n",
        "\n",
        "\n",
        "* ƒê·ªçc l√† m·ªôt k·ªπ nƒÉng c·∫ßn thi·∫øt ƒë·ªÉ th√†nh c√¥ng trong h·ªçc t·∫≠p, Khi h·ªçc sinh ti·∫øp c·∫≠n ƒë∆∞·ª£c nh·ªØng ƒëo·∫°n vƒÉn v·ªõi m·ª©c ƒë·ªô th√°ch th·ª©c ph√π h·ª£p, ch√∫ng s·∫Ω ph√°t tri·ªÉn k·ªπ nƒÉng ƒë·ªçc m·ªôt c√°ch t·ª± nhi√™n.\n",
        "\n",
        "* CommonLit l√† m·ªôt t·ªï ch·ª©c phi l·ª£i nhu·∫≠n ph·ª•c v·ª• h∆°n 20 tr gi√°o vi√™n v√† h·ªçc sinh v·ªõi c√°c b√†i h·ªçc ƒë·ªçc v√† vi·∫øt kƒ© thu·∫≠t s·ªë mi·ªÖn ph√≠ cho c√°c l·ªõp 3-12, c√πng v·ªõi ƒë·∫°i h·ªçc bang Georgia, m·ªôt tr∆∞·ªùng ƒë·∫°i h·ªçc nghi√™n c·ª©u ·ªü Atlanta, ƒë√£ t·ªï ch·ª©c 1 cu·ªôc thi nh·∫±m c·∫£i thi·ªán ph∆∞∆°ng ph√°p ƒë√°nh gi√° kh·∫£ nƒÉng ƒë·ªçc."
      ],
      "id": "TTgFA0XYrcWo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAZscSiIrgE6"
      },
      "source": [
        "\n",
        "\n",
        "### N·ªôi dung\n",
        "\n",
        "\n",
        "*   Trong cu·ªôc th·ªã n√†y, ng∆∞·ªùi tham gia s·∫Ω x√¢y d·ª±ng c√°c thu·∫≠t to√°n ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô ph·ª©c t·∫°p c·ªßa vi·ªác ƒë·ªçc c√°c ƒëo·∫°n vƒÉn ƒë·ªÉ s·ª≠ d·ª•ng trong l·ªõp h·ªçc t·ª´ l·ªõp 3-12. ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c ƒëi·ªÅu n√†y, ph·∫£i k·∫øt h·ª£p kƒ© nƒÉng h·ªçc m√°y c·ªßa h·ªç ƒë·ªÉ l√†m vi·ªác v·ªõi 1 t·∫≠p d·ªØ li·ªáu bao g·ªìm ng∆∞·ªùi ƒë·ªçc t·ª´ nhi·ªÅu nh√≥m tu·ªïi kh√°c nhau v·ªõi m·ªôt s·ªë l∆∞·ª£ng l·ªõn c√°c vƒÉn b·∫£n thu·ªôc nhi·ªÅu lƒ©nh v·ª±c kh√°c nhau.\n",
        "\n",
        "\n",
        "*   N·∫øu th√†nh c√¥ng, b·∫°n ƒë√£ h·ªó tr·ª£ c√°c gi√°o vi√™n v√† sinh vi√™n, c√°c nh√† ph√°t tri·ªÅn ch∆∞∆°ng tr√¨nh gi·∫£ng d·∫°y ƒë·ªçc vi·∫øt c√≥ th·ªÉ ƒë√°nh gi√° nhanh ch√≥ng v√† ch√≠nh x√°c c√°c t√†i li·ªáu trong l·ªõp h·ªçc c·ªßa h·ªç. Quan tr·ªçng nh·∫•t, h·ªçc sinh s·∫Ω ƒë∆∞·ª£c h∆∞·ªüng l·ª£i t·ª´ vi·ªác ƒë√°nh gi√° ƒë·ªô ph·ª©c t·∫°p v√† kh·∫£ nƒÉng ƒë·ªçc c·ªßa h·ªç, gi√∫p h·ªç c·∫£i thi·ªán c√°c k·ªπ nƒÉng ƒë·ªçc thi·∫øt y·∫øu d·ªÖ d√†ng h∆°n nhi·ªÅu."
      ],
      "id": "cAZscSiIrgE6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWhI03-Upio"
      },
      "source": [
        "\n",
        "\n",
        "### ƒê√°nh gi√°\n",
        "\n",
        "-\tC√°c b√†i d·ª± thi s·∫Ω ƒë∆∞·ª£c ƒë√°nh gi√° d·ª±a tr√™n ƒë·ªô l·ªói RMSE: \n",
        "\n",
        "    ![image.png](https://scontent.fdad1-1.fna.fbcdn.net/v/t1.15752-9/258845405_591261328774655_4260656527898854926_n.png?_nc_cat=109&ccb=1-5&_nc_sid=ae9488&_nc_ohc=JcFQuPsmYnYAX-OYyFs&_nc_ht=scontent.fdad1-1.fna&oh=491e243d6da6d57179a18d30529a8466&oe=61BF2150)\n",
        "\n",
        "\n",
        "### Submission file:\n",
        "-\tBao g·ªìm ID v√† ƒë·ªô d·ªÖ ƒë·ªçc c·ªßa ƒëo·∫°n tr√≠ch:\n",
        "\n",
        "    ![image.png](https://scontent.fdad1-2.fna.fbcdn.net/v/t1.15752-9/254632714_433915911583493_5614680366254606947_n.png?_nc_cat=102&ccb=1-5&_nc_sid=ae9488&_nc_ohc=omTcknGdud8AX8hu4Fv&_nc_ht=scontent.fdad1-2.fna&oh=9ad8860d2bb51f4ab489fecd599a056d&oe=61BECC2F)\n"
      ],
      "id": "BBWhI03-Upio"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOmdwmnwr_JQ"
      },
      "source": [
        "### Chi ti·∫øt d·ªØ li·ªáu\n",
        "  1. \tId:  m√£ id c·ªßa ƒëo·∫°n tr√≠ch (unique)\n",
        "  2.   url_legal: ƒë∆∞·ªùng d·∫´n c·ªßa ƒëo·∫°n tr√≠ch- tr∆∞·ªùng n√†y c√≥ th·ªÉ tr·ªëng trong t·∫≠p test\n",
        "  3.   license: b·∫£n quy·ªÅn c·ªßa ƒëo·∫°n tr√≠ch- tr∆∞·ªùng n√†y c√≥ th·ªÉ tr·ªëng trong t·∫≠p test\n",
        "  4.   excerpt: n·ªôi dung ƒëo·∫°n tr√≠ch c·∫ßn d·ª± ƒëo√°n ƒë·ªô d·ªÖ ƒë·ªçc\n",
        "  5.   target: ƒë·ªô d·ªÖ c·ªßa ƒëo·∫°n tr√≠ch (ƒë∆∞·ª£c l·∫•y t·ª´ nhi·ªÅu lƒ©nh v·ª±c v√† ƒë∆∞·ª£c nhi·ªÅu ng∆∞·ªùi ·ªü c√°c ƒë·ªô tu·ªïi kh√°c nh·∫°u ƒë√°nh gi√°)\n",
        "  6.   standard_error: th∆∞·ªõc ƒëo ch√™nh l·ªách ƒëi·ªÉm s·ªë gi·ªØa nhi·ªÅu ng∆∞·ªùi ƒë√°nh gi√° cho m·ªói ƒëo·∫°n tr√≠ch. Kh√¥ng ƒë∆∞·ª£c bao g·ªìm trong t·∫≠p test."
      ],
      "id": "fOmdwmnwr_JQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d48122eb"
      },
      "source": [
        "## Gi·∫£i quy·∫øt b√†i to√°n\n",
        "[Gi·∫£i ph√°p](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/258148) - ƒê·ª©ng th·ª© 4 trong private leaderboard (üèÖÔ∏è 4th Place Solution (0.447) üèÖÔ∏è)"
      ],
      "id": "d48122eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "483e6c5d"
      },
      "source": [
        "#### L·∫•y d·ªØ li·ªáu\n"
      ],
      "id": "483e6c5d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWHdWNQGsomY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61102403-1341-4ad3-8dac-2083e6c00536"
      },
      "source": [
        "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1_A5Y2u2XjFVfxVBr6flozZ-Gr0gKRaGm\" -O commonlitreadabilityprize.zip &> /dev/null\n",
        "!mkdir -p input_data/commonlitreadabilityprize\n",
        "!mkdir -p output/commonlitreadabilityprize\n",
        "!mkdir commonlitreadabilityprize\n",
        "!unzip /content/commonlitreadabilityprize.zip -d input_data/commonlitreadabilityprize/"
      ],
      "id": "zWHdWNQGsomY",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/commonlitreadabilityprize.zip\n",
            "  inflating: input_data/commonlitreadabilityprize/sample_submission.csv  \n",
            "  inflating: input_data/commonlitreadabilityprize/test.csv  \n",
            "  inflating: input_data/commonlitreadabilityprize/train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKBcxZUR2q9e"
      },
      "source": [
        "#### EDA"
      ],
      "id": "NKBcxZUR2q9e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv7xVGEs2zHl"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "Sv7xVGEs2zHl",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "m3fhqiVV3O40",
        "outputId": "09904b46-ed0e-497d-a721-598f9817405b"
      },
      "source": [
        "data_train = pd.read_csv(\"train.csv\",index_col=0)\n",
        "data_train.head(3)"
      ],
      "id": "m3fhqiVV3O40",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>c12129c31</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When the young people returned to the ballroom...</td>\n",
              "      <td>-0.340259</td>\n",
              "      <td>0.464009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85aa80a4c</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
              "      <td>-0.315372</td>\n",
              "      <td>0.480805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b69ac6792</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>As Roger had predicted, the snow departed as q...</td>\n",
              "      <td>-0.580118</td>\n",
              "      <td>0.476676</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          url_legal license  ...    target  standard_error\n",
              "id                           ...                          \n",
              "c12129c31       NaN     NaN  ... -0.340259        0.464009\n",
              "85aa80a4c       NaN     NaN  ... -0.315372        0.480805\n",
              "b69ac6792       NaN     NaN  ... -0.580118        0.476676\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs0TE0RwOYIO",
        "outputId": "98cf39c6-5103-4bb1-c445-2521de00a6ea"
      },
      "source": [
        "data_train = data_train.dropna(axis = 1)\n",
        "data_train.shape"
      ],
      "id": "fs0TE0RwOYIO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2834, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vyYSSTrmOO49",
        "outputId": "13f3b182-eb59-4322-a699-200c2263d7c5"
      },
      "source": [
        "data_train.describe()"
      ],
      "id": "vyYSSTrmOO49",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2834.000000</td>\n",
              "      <td>2834.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.959319</td>\n",
              "      <td>0.491435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.033579</td>\n",
              "      <td>0.034818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.676268</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.690320</td>\n",
              "      <td>0.468543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.912190</td>\n",
              "      <td>0.484721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>-0.202540</td>\n",
              "      <td>0.506268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.711390</td>\n",
              "      <td>0.649671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            target  standard_error\n",
              "count  2834.000000     2834.000000\n",
              "mean     -0.959319        0.491435\n",
              "std       1.033579        0.034818\n",
              "min      -3.676268        0.000000\n",
              "25%      -1.690320        0.468543\n",
              "50%      -0.912190        0.484721\n",
              "75%      -0.202540        0.506268\n",
              "max       1.711390        0.649671"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWUOvpGGnnnr"
      },
      "source": [
        "* Gi√° tr·ªã nh·ªè nh·∫•t c·ªôt ƒë·ªô l·ªách chu·∫©n c√≥ gi√° tr·ªã 0. Nguy√™n nh√¢n c√≥ th·ªÉ l√† ƒëo·∫°n tr√≠ch ƒë∆∞·ª£c √≠t ng∆∞·ªùi ƒë√°nh gi√°. "
      ],
      "id": "xWUOvpGGnnnr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "rOziLnKyNBTI",
        "outputId": "ad33d046-b7a4-4a5b-e860-26740dd58df5"
      },
      "source": [
        "df = data_train\n",
        "n_bins = 20\n",
        "fig, axs = plt.subplots(1, figsize=(5, 5), sharey=True, tight_layout=True)\n",
        "\n",
        "axs.hist(df.target, bins=n_bins);\n",
        "# axs[1].hist(df.standard_error, bins=n_bins);"
      ],
      "id": "rOziLnKyNBTI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT7SLn5WV3GJ"
      },
      "source": [
        "* Nh·∫≠n x√©t :\n",
        "  - ƒêa ph·∫ßn c√°c ƒëo·∫°n text ƒë·ªÅu trong t·∫≠p train ƒë·ªÅu ·ªü m·ª©c ƒë·ªô kh√≥."
      ],
      "id": "vT7SLn5WV3GJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "R-dn2bLqhyYT",
        "outputId": "e921cee9-bae4-40ad-fe0a-237b9a275b5e"
      },
      "source": [
        "fig, axs = plt.subplots(1, figsize=(5, 5), sharey=True, tight_layout=True)\n",
        "\n",
        "axs.hist(df.standard_error, bins=n_bins);"
      ],
      "id": "R-dn2bLqhyYT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNp4W-91FvG"
      },
      "source": [
        "* Nh·∫≠n x√©t: ƒê·ªô l·ªách chu·∫©n c·ªßa c√°c ƒë√°nh gi√° ƒëa ph·∫ßn giao ƒë·ªông trong kho·∫£ng 0.5"
      ],
      "id": "ypNp4W-91FvG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "EaazXyVcVHrB",
        "outputId": "9a383d88-a2ac-4445-f2f0-6ffdef76c083"
      },
      "source": [
        "df[\"word_count\"] = df['excerpt'].str.split().apply(len)\n",
        "df[\"word_count\"].hist();"
      ],
      "id": "EaazXyVcVHrB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW1klEQVR4nO3df5DcdX3H8edLfpl6NhHBbZpkekyN7VBujGSLOPbHHrQKsdNgqwxMRoPSuWqxg/W0BNuptNZpbBuZWi3t2VBitR4UtaQIbTFyZZxpwBwGLuGHHnK0OWMySgyeUuzhu3/sJ8NybG73bndv9/vh9ZjZ2e/38/l8d1+32bzvu9/77PeriMDMzPLygm4HMDOz9nNxNzPLkIu7mVmGXNzNzDLk4m5mlqETux0A4LTTTov+/v5ux3iW73//+7zoRS/qdoymFSlvkbJCsfIWKSsUK28vZh0fH/92RJxer68nint/fz979uzpdoxnGRsbo1KpdDtG04qUt0hZoVh5i5QVipW3F7NKeux4fT4sY2aWIRd3M7MMubibmWXIxd3MLEMu7mZmGXJxNzPLkIu7mVmGXNzNzDLk4m5mlqGe+IaqmT1X/5YvLHib4YFZLlvEdrWmtr6hpe2tN3jP3cwsQy7uZmYZcnE3M8tQ08Vd0gmSvirp1rR+hqS7JU1KulHSyan9lLQ+mfr7OxPdzMyOZyF77lcCD9asfxi4NiJeDhwBLk/tlwNHUvu1aZyZmS2hpoq7pNXAG4C/T+sCzgNuTkN2ABel5Y1pndR/fhpvZmZLRBHReJB0M/BnwIuB9wKXAbvT3jmS1gC3R8RZkvYBF0TEgdT3CPDqiPj2nMccAoYASqXS+tHR0bb9UO0wMzNDX19ft2M0rUh5i5QVupd3YvrogrcpLYNDT7b2vAOrlrf2AAtQpPdCL2YdHBwcj4hyvb6G89wl/RpwOCLGJVXaFSoiRoARgHK5HL12hZNevOrKfIqUt0hZoXt5FzNffXhglm0TrX19ZWpTpaXtF6JI74UiZYXmvsT0WuDXJW0AXgj8OPBXwApJJ0bELLAamE7jp4E1wAFJJwLLge+0PbmZmR1Xw2PuEXF1RKyOiH7gEuBLEbEJuBN4Uxq2GbglLe9M66T+L0Uzx37MzKxtWpnnfhXwHkmTwEuB7al9O/DS1P4eYEtrEc3MbKEWdHAuIsaAsbT8DeCcOmP+F3hzG7KZmdki+RuqZmYZcnE3M8uQi7uZWYZc3M3MMuTibmaWIRd3M7MM+TJ7ZtYzFnNpwXbI8dKC3nM3M8uQi7uZWYZc3M3MMuTibmaWIRd3M7MMubibmWXIUyHN7FmWcjri8MDsoq44ZY15z93MLEMu7mZmGWpY3CW9UNI9ku6TtF/SH6f2GyQ9Kmlvuq1L7ZL0UUmTku6XdHanfwgzM3u2Zo65PwWcFxEzkk4Cvizp9tT3voi4ec74C4G16fZq4Lp0b2ZmS6SZC2RHRMyk1ZPSbb4LXm8EPpm22w2skLSy9ahmZtYsRcxXp9Mg6QRgHHg58PGIuErSDcBrqO7Z7wK2RMRTkm4FtkbEl9O2u4CrImLPnMccAoYASqXS+tHR0fb9VG0wMzNDX19ft2M0rUh5i5QV4PDjRzn0ZLdTNKe0jMJkhd7JO7BqecMxvfi+HRwcHI+Icr2+pqZCRsTTwDpJK4DPSzoLuBr4FnAyMAJcBfxJs6EiYiRtR7lcjkql0uymS2JsbIxeyzSfIuUtUlaAv/70LWybKMas4eGB2cJkhd7JO7Wp0nBM0d63C5otExHfBe4ELoiIg+nQy1PAPwDnpGHTwJqazVanNjMzWyLNzJY5Pe2xI2kZ8KvAQ8eOo0sScBGwL22yE3hrmjVzLnA0Ig52JL2ZmdXVzOehlcCOdNz9BcBNEXGrpC9JOh0QsBd4Rxp/G7ABmAR+ALyt/bHNzGw+DYt7RNwPvKpO+3nHGR/AFa1HMzOzxfI3VM3MMuTibmaWIRd3M7MMubibmWXIxd3MLEMu7mZmGXJxNzPLkIu7mVmGXNzNzDLk4m5mliEXdzOzDLm4m5llyMXdzCxDLu5mZhnq/vWtzJrQv+ULXXvu4YGuPbXZonnP3cwsQ81cZu+Fku6RdJ+k/ZL+OLWfIeluSZOSbpR0cmo/Ja1Ppv7+zv4IZmY2VzN77k8B50XEK4F1wAXp2qgfBq6NiJcDR4DL0/jLgSOp/do0zszMllDD4h5VM2n1pHQL4Dzg5tS+g+pFsgE2pnVS//npItpmZrZEVL3kaYNB1YtjjwMvBz4O/AWwO+2dI2kNcHtEnCVpH3BBRBxIfY8Ar46Ib895zCFgCKBUKq0fHR1t30/VBjMzM/T19XU7RtOKlHcxWSemj3YoTWOlZXDoya49/YIUKSv0Tt6BVcsbjunF/2ODg4PjEVGu19fUbJmIeBpYJ2kF8HngZ1sNFREjwAhAuVyOSqXS6kO21djYGL2WaT5FyruYrJd1dbbMLNsmijGxrEhZoXfyTm2qNBxTpP9jsMCpkBHxXUl3Aq8BVkg6MSJmgdXAdBo2DawBDkg6EVgOfKeNma2L2jElcXhgtqvF2uz5oJnZMqenPXYkLQN+FXgQuBN4Uxq2GbglLe9M66T+L0Uzx37MzKxtmtlzXwnsSMfdXwDcFBG3SnoAGJX0p8BXge1p/HbgHyVNAo8Dl3Qgt5mZzaNhcY+I+4FX1Wn/BnBOnfb/Bd7clnRmZrYo/oaqmVmGXNzNzDLk4m5mliEXdzOzDLm4m5llyMXdzCxDLu5mZhnq/kkdbMHqnQLAX+k3s1reczczy5CLu5lZhlzczcwy5OJuZpYhF3czswy5uJuZZcjF3cwsQy7uZmYZauYye2sk3SnpAUn7JV2Z2q+RNC1pb7ptqNnmakmTkh6W9PpO/gBmZvZczXxDdRYYjoh7Jb0YGJd0R+q7NiL+snawpDOpXlrv54CfBL4o6RUR8XQ7g5uZ2fE13HOPiIMRcW9a/h7Vi2OvmmeTjcBoRDwVEY8Ck9S5HJ+ZmXWOIqL5wVI/cBdwFvAe4DLgCWAP1b37I5I+BuyOiE+lbbYDt0fEzXMeawgYAiiVSutHR0db/VnaamZmhr6+vm7HqGti+uhz2krL4NCTXQizCEXKCsXKW6Ss0Dt5B1YtbzimF2vC4ODgeESU6/U1feIwSX3AZ4F3R8QTkq4DPghEut8GvL3Zx4uIEWAEoFwuR6VSaXbTJTE2NkavZTqm3gnChgdm2TZRjPPAFSkrFCtvkbJC7+Sd2lRpOKaXa0I9Tc2WkXQS1cL+6Yj4HEBEHIqIpyPiR8AneObQyzSwpmbz1anNzMyWSDOzZQRsBx6MiI/UtK+sGfZGYF9a3glcIukUSWcAa4F72hfZzMwaaebz0GuBtwATkvamtvcDl0paR/WwzBTw2wARsV/STcADVGfaXOGZMmZmS6thcY+ILwOq03XbPNt8CPhQC7nMzKwF/oaqmVmGXNzNzDLk4m5mliEXdzOzDLm4m5llyMXdzCxDLu5mZhlycTczy5CLu5lZhlzczcwy5OJuZpYhF3czswy5uJuZZaj7l0ApsP46V0QyM+sF3nM3M8uQi7uZWYaauczeGkl3SnpA0n5JV6b2UyXdIenr6f4lqV2SPippUtL9ks7u9A9hZmbP1sye+ywwHBFnAucCV0g6E9gC7IqItcCutA5wIdXrpq4FhoDr2p7azMzm1bC4R8TBiLg3LX8PeBBYBWwEdqRhO4CL0vJG4JNRtRtYMedi2mZm1mGKiOYHS/3AXcBZwH9HxIrULuBIRKyQdCuwNV17FUm7gKsiYs+cxxqiumdPqVRaPzo62vpP00YzMzP09fXNO2Zi+ugSpWmstAwOPdntFM0pUlYoVt4iZYXeyTuwannDMc3UhKU2ODg4HhHlen1NT4WU1Ad8Fnh3RDxRredVERGSmv8tUd1mBBgBKJfLUalUFrJ5x42NjdEo02U9NBVyeGCWbRPFmNlapKxQrLxFygq9k3dqU6XhmGZqQi9paraMpJOoFvZPR8TnUvOhY4db0v3h1D4NrKnZfHVqMzOzJdLMbBkB24EHI+IjNV07gc1peTNwS037W9OsmXOBoxFxsI2ZzcysgWY+D70WeAswIWlvans/sBW4SdLlwGPAxanvNmADMAn8AHhbWxObmVlDDYt7+sOojtN9fp3xAVzRYi4zM2uBv6FqZpYhF3czswy5uJuZZcjF3cwsQy7uZmYZcnE3M8uQi7uZWYZc3M3MMuTibmaWoe6fjs3MrMuaudj98MBsR84EO7X1DW1/TPCeu5lZllzczcwy5OJuZpYhF3czswy5uJuZZcjF3cwsQ81cZu96SYcl7atpu0bStKS96bahpu9qSZOSHpb0+k4FNzOz42tmz/0G4II67ddGxLp0uw1A0pnAJcDPpW3+RtIJ7QprZmbNaVjcI+Iu4PEmH28jMBoRT0XEo1Svo3pOC/nMzGwRVL3kaYNBUj9wa0ScldavAS4DngD2AMMRcUTSx4DdEfGpNG47cHtE3FznMYeAIYBSqbR+dHS0DT9O+8zMzNDX1zfvmInpo0uUprHSMjj0ZLdTNKdIWaFYeYuUFYqVt1NZB1YtX/S2g4OD4xFRrte32NMPXAd8EIh0vw14+0IeICJGgBGAcrkclUplkVE6Y2xsjEaZOvFV5MUaHphl20QxziZRpKxQrLxFygrFytuprFObKm1/TFjkbJmIOBQRT0fEj4BP8Myhl2lgTc3Q1anNzMyW0KKKu6SVNatvBI7NpNkJXCLpFElnAGuBe1qLaGZmC9XwM4akzwAV4DRJB4APABVJ66gelpkCfhsgIvZLugl4AJgFroiIpzsT3czMjqdhcY+IS+s0b59n/IeAD7USyszMWuNvqJqZZcjF3cwsQy7uZmYZcnE3M8uQi7uZWYZc3M3MMuTibmaWIRd3M7MMubibmWXIxd3MLEMu7mZmGXJxNzPLkIu7mVmGXNzNzDLk4m5mliEXdzOzDDUs7pKul3RY0r6atlMl3SHp6+n+Jaldkj4qaVLS/ZLO7mR4MzOrr5k99xuAC+a0bQF2RcRaYFdaB7iQ6nVT1wJDwHXtiWlmZgvRsLhHxF3A43OaNwI70vIO4KKa9k9G1W5gxZyLaZuZ2RJQRDQeJPUDt0bEWWn9uxGxIi0LOBIRKyTdCmyNiC+nvl3AVRGxp85jDlHdu6dUKq0fHR1tz0/UJjMzM/T19c07ZmL66BKlaay0DA492e0UzSlSVihW3iJlhWLl7VTWgVXLF73t4ODgeESU6/U1vEB2IxERkhr/hnjudiPACEC5XI5KpdJqlLYaGxujUabLtnxhacI0YXhglm0TLf9zLokiZYVi5S1SVihW3k5lndpUaftjwuJnyxw6drgl3R9O7dPAmppxq1ObmZktocUW953A5rS8Gbilpv2tadbMucDRiDjYYkYzM1ughp8xJH0GqACnSToAfADYCtwk6XLgMeDiNPw2YAMwCfwAeFsHMpuZWQMNi3tEXHqcrvPrjA3gilZDmZlZa/wNVTOzDLm4m5llyMXdzCxDLu5mZhlycTczy5CLu5lZhlzczcwy5OJuZpYhF3czswwV43Rs8+jv0JkZhwdme+qsj2ZmC+E9dzOzDLm4m5llyMXdzCxDLu5mZhlycTczy5CLu5lZhlqaCilpCvge8DQwGxFlSacCNwL9wBRwcUQcaS2mmZktRDv23AcjYl1ElNP6FmBXRKwFdqV1MzNbQp04LLMR2JGWdwAXdeA5zMxsHqpe9nSRG0uPAkeAAP4uIkYkfTciVqR+AUeOrc/ZdggYAiiVSutHR0cXlWFi+uhi48+rtAwOPdmRh+6IIuUtUlYoVt4iZYVi5e1U1oFVyxe97eDg4HjNUZNnafX0A78QEdOSXgbcIemh2s6ICEl1f3tExAgwAlAul6NSqSwqQKdOETA8MMu2ieKcnaFIeYuUFYqVt0hZoVh5O5V1alOl7Y8JLR6WiYjpdH8Y+DxwDnBI0kqAdH+41ZBmZrYwiy7ukl4k6cXHloHXAfuAncDmNGwzcEurIc3MbGFa+YxRAj5fPazOicA/RcS/SfoKcJOky4HHgItbj2lmZgux6OIeEd8AXlmn/TvA+a2EMjOz1vgbqmZmGXJxNzPLkIu7mVmGXNzNzDLk4m5mliEXdzOzDLm4m5llyMXdzCxDLu5mZhlycTczy5CLu5lZhlzczcwy5OJuZpYhF3czswy5uJuZZcjF3cwsQx0r7pIukPSwpElJWzr1PGZm9lwdKe6STgA+DlwInAlcKunMTjyXmZk9V6f23M8BJiPiGxHxQ2AU2Nih5zIzszkUEe1/UOlNwAUR8Vtp/S3AqyPiXTVjhoChtPozwMNtD9Ka04BvdzvEAhQpb5GyQrHyFikrFCtvL2b9qYg4vV7Hoi+Q3aqIGAFGuvX8jUjaExHlbudoVpHyFikrFCtvkbJCsfIWKSt07rDMNLCmZn11ajMzsyXQqeL+FWCtpDMknQxcAuzs0HOZmdkcHTksExGzkt4F/DtwAnB9ROzvxHN1UM8eMjqOIuUtUlYoVt4iZYVi5S1S1s78QdXMzLrL31A1M8uQi7uZWYaet8Vd0vWSDkvaV6dvWFJIOi2tS9JH06kU7pd0drezSrpG0rSkvem2oabv6pT1YUmvX8qsx8ub2n9X0kOS9kv6817Ie5zX9saa13VK0t5eyDpP3nWSdqe8eySdk9p78X37Skn/JWlC0r9K+vGavm6+D9ZIulPSA+n9eWVqP1XSHZK+nu5fktq7+to2JSKelzfgl4CzgX1z2tdQ/UPwY8BpqW0DcDsg4Fzg7m5nBa4B3ltn7JnAfcApwBnAI8AJPZB3EPgicEpaf1kv5D3e+6CmfxvwR72QdZ7X9j+AC2veq2M9/L79CvDLafntwAd74bUFVgJnp+UXA19Lmf4c2JLatwAf7oXXtpnb83bPPSLuAh6v03Ut8PtA7V+aNwKfjKrdwApJK5cgJjBv1no2AqMR8VREPApMUj0dxJI5Tt53Alsj4qk05nBq72re+V5bSQIuBj6Tmnr1tQ3g2B7wcuCbabkX37evAO5Ky3cAv5mWu/0+OBgR96bl7wEPAqtSrh1p2A7gopq8XXttm/G8Le71SNoITEfEfXO6VgH/U7N+ILV127vSR8Lrj31cpHezvgL4RUl3S/pPST+f2ns1L8AvAoci4utpvVezvhv4C0n/A/wlcHVq78W8+3nmPFNv5pkvO/ZMVkn9wKuAu4FSRBxMXd8CSmm5Z/Iej4t7IunHgPcDf9TtLE26DvhpYB1wkOrhg152InAq1Y+w7wNuSnvGvexSntlr72XvBH4vItYAvwds73Ke+bwd+B1J41QPf/ywy3meRVIf8Fng3RHxRG1fVI/HFGbuuIv7M36a6rG++yRNUT1lwr2SfoIePJ1CRByKiKcj4kfAJ3jmI2zPZU0OAJ9LH2PvAX5E9URMPZlX0onAbwA31jT3ZFZgM/C5tPzP9PB7ISIeiojXRcR6qr84H0ldXc8q6SSqhf3TEXHs9Tx07HBLuj92OLHreRtxcU8iYiIiXhYR/RHRT7UYnR0R36J66oS3pr+Qnwscrfmo1hVzju+9ETg2I2EncImkUySdAawF7lnqfHX8C9U/qiLpFcDJVM+w16t5fwV4KCIO1LT1atZvAr+cls8Djh1G6sX37cvS/QuAPwT+NnV19bVNnyK3Aw9GxEdqunZS/eVJur+lpr2nXtvn6PZfdLt1o7rXcBD4P6qF/PI5/VM8M1tGVC8+8ggwAZS7nRX4x5TlfqpvtJU14/8gZX2YNIuiB/KeDHyK6i+he4HzeiHv8d4HwA3AO+qM78XX9heAcaqzTe4G1vfw+/ZKqjNRvgZsJX1LvtuvbXoNI/1/2ptuG4CXAruo/sL8InBqL7y2zdx8+gEzswz5sIyZWYZc3M3MMuTibmaWIRd3M7MMubibmWXIxd3MLEMu7mZmGfp/DYlZrtqn4yIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPdl7X0sm2ie"
      },
      "source": [
        "* Nh·∫≠n x√©t: Ph·∫ßn l·ªõn ƒë·ªô d√†i c√°c ƒëo·∫°n tr√≠ch n·∫±m trong kho·∫£ng t·ª´ 300-400."
      ],
      "id": "JPdl7X0sm2ie"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmlc-KxpaGSA",
        "outputId": "032dee21-8c3c-4771-df6f-4e214ee5a9ff"
      },
      "source": [
        " total = df['excerpt'].str.split().explode().value_counts().sum()\n",
        " stopwords = df['excerpt'].str.split().explode().value_counts().nlargest(50).sum()\n",
        " print(len(df['excerpt'].str.split().explode().unique()))\n",
        " print((stopwords / total) * 100)"
      ],
      "id": "Pmlc-KxpaGSA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54381\n",
            "39.02792765043419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "VThN-ownaYzc",
        "outputId": "3ba09e5a-b605-4dae-e02f-c0fd72c983ff"
      },
      "source": [
        "plt.figure(figsize = (20, 6), dpi = 80)\n",
        "df['excerpt'].str.split().explode().value_counts().nlargest(50).plot.bar(rot = 0)\n",
        "plt.title(\"T·∫ßn su·∫•t c·ªßa c√°c t·ª´ trong t·∫≠p huy·∫øn luy·ªán\");"
      ],
      "id": "VThN-ownaYzc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1600x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6o_5sWd1UE"
      },
      "source": [
        "* Nh·∫≠n x√©t: \n",
        " - Gi·ªëng nh∆∞ mong ƒë·ª£i th√¨ c√°c t·ª´ stopwords chi·∫øm ph·∫ßn l·ªõn t·ª´ trong d·ªØ li·ªáu. \n",
        " - Stopword Chi·∫øm kho·∫£ng 40% trong t·ªïng s·ªë l∆∞·ª£ng t·ª´"
      ],
      "id": "PU6o_5sWd1UE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouZ-KOTwa9_H"
      },
      "source": [
        "### T√≥m t·∫Øt v·ªÅ gi·∫£i ph√°p\n",
        "#### T·ªïng qu√°t \n",
        "* T√°c gi·∫£ s·ª≠ d·ª•ng k·ªπ thu·∫≠t 5 fold CV v·ªõi 5 seed kh√°c nhau ƒë·ªÉ t·∫°o ra nhi·ªÅu checkpoint kh√°c nhau.\n",
        "* V√† √°p d·ª•ng v·ªõi nhi·ªÅu m√¥ h√¨nh kh√°c nhau (T√°c gi·∫£ s·ª≠ d·ª•ng kho·∫£ng 38 m√¥ h√¨nh).\n",
        "* K·∫øt qu·∫£ qu·∫£ cu·ªëi c√πng t√°c gi·∫£ t·ªïng h·ª£p l·∫°i v·ªõi 3 c√°ch ch√≠nh:\n",
        "  - RigdeCV (LOO) \n",
        "  - BayesianRidgeRegression\n",
        "  - BigChaos\n",
        "* ƒê·ªìng th·ªùi √°p d·ª•ng m·ªôt s·ªë kƒ© thu·∫≠t kh√°c nh∆∞ th√™m c√°c thu·ªôc t√≠nh (flesch_reading_ease ,smog_index) , th√™m AttentionBlock."
      ],
      "id": "ouZ-KOTwa9_H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXPOK1RUX9le"
      },
      "source": [
        "#### Th√™m thu·ªôc t√≠nh\n"
      ],
      "id": "hXPOK1RUX9le"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29tyTz7uYAs1"
      },
      "source": [
        "* flesch_reading_ease l√† ƒëi·ªÉm d·ªÖ ƒë·ªçc c·ªßa ƒëo·∫°n tr√≠ch.\n",
        "  - C√¥ng th·ª©c: \n",
        "\n",
        "    ![picture](https://readable.com/wp-content/uploads/2017/01/FLESCHREADINGEASE.png)\n",
        "\n",
        "  - ƒê√°nh gi√° k·∫øt qu·∫£:\n",
        "\n",
        "    ![picture](https://seodesignchicago.com/wp-content/uploads/2021/09/Flesch-Reading-Ease.png)\n",
        "* smog_index l√† ∆∞·ªõc l∆∞·ª£ng s·ªë nƒÉm h·ªçc ƒë·ªÉ ƒë·ªçc ƒëo·∫°n tr√≠ch.\n",
        "  - C√¥ng th·ª©c:\n",
        "\n",
        "    ![picture](https://readable.com/wp-content/uploads/2019/01/SMOG-readability-formula-1-768x304.png)\n",
        "\n",
        "  - V√≠ d·ª•: k·∫øt qu·∫£ l√† 9 th√¨ h·ªçc sinh l·ªõp 9 c√≥ th·ªÉ ƒë·ªçc ƒëo·∫°n tr√≠ch ƒë√≥."
      ],
      "id": "29tyTz7uYAs1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZRCzUMeWKdK"
      },
      "source": [
        "#### Ph∆∞∆°ng th·ª©c t·ªïng h·ª£p\n",
        " "
      ],
      "id": "YZRCzUMeWKdK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQu3HXLXrtd"
      },
      "source": [
        "- RigdeCV:     \n",
        "    X√¢y d·ª±ng m·ªôt m√¥ h√¨nh h·ªìi quy c√≥ k·∫øt h·ª£p k·ªπ thu·∫≠t CV \n",
        "- BayesianRidgeRegression: \n",
        "\n",
        "    BayesianRidge x√¢y d·ª±ng h·ªìi quy tuy·∫øn t√≠nh s·ª≠ d·ª•ng ph√¢n ph·ªëi x√°c su·∫•t thay v√¨ ∆∞·ªõc l∆∞·ª£ng ƒëi·ªÉm.\n",
        "\n",
        "    RidgeRegression l√† m·ªôt k·ªπ thu·∫≠t ƒë·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu h·ªìi quy nhi·ªÅu l·∫ßn. \n",
        "\n",
        "    M·ªôt trong nh·ªØng lo·∫°i H·ªìi quy Bayesian hi·ªáu qu·∫£ l√† Bayesian Ridge Regression, ∆∞·ªõc t√≠nh m√¥ h√¨nh x√°c su·∫•t cho c√°c b√†i to√°n h·ªìi quy.\n",
        "\n",
        "    LOOCV l√† m·ªôt ph∆∞∆°ng ph√°p Cross Validation: Chia t·∫≠p d·ªØ li·ªáu th√†nh K t·∫≠p con (K = s·ªë d·ªØ li·ªáu). Qu√° tr√¨nh h·ªçc c·ªßa m√°y c√≥ K l·∫ßn. M·ªói l·∫ßn, 1 t·∫≠p con ƒë∆∞·ª£c d√πng ƒë·ªÉ ki·ªÉm tra v√† K-1 t·∫≠p con c√≤n l·∫°i ƒë∆∞·ª£c d√πng ƒë·ªÉ hu·∫•n luy·ªán.\n",
        "- Bigchaos:    \n",
        "    Big chaos solution l√† m·ªôt ph∆∞∆°ng ph√°p Collaborative filtering ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng trong b√†i to√°n Recommender system.\n",
        "\n",
        "    ![image.png](https://scontent.fdad1-2.fna.fbcdn.net/v/t1.15752-9/257430565_582013389559613_5705009368260678979_n.png?_nc_cat=102&ccb=1-5&_nc_sid=ae9488&_nc_ohc=YNARF7N0gU8AX_ellxS&_nc_ht=scontent.fdad1-2.fna&oh=69e274b018003c81d4338c57cfccfe91&oe=61C2D4AC)\n",
        "    Trong ƒë√≥ thu·∫≠t to√°n s·∫Ω d·ª± ƒëo√°n ratings c·ªßa nh·ªØng b·ªô phim m√† ng∆∞·ªùi d√πng ch∆∞a xem, c√°c phim c√≥ ƒëi·ªÉm ratings cao s·∫Ω ƒë∆∞·ª£c recommend cho ng∆∞·ªùi d√πng.\n",
        "    Tuy ƒë·∫°t ƒë·ªô l·ªói nh·ªè h∆°n (0.446) nh∆∞ng t√°c tin t∆∞·ªüng 2 ph∆∞∆°ng ph√°p RigdeCV v√† BayesianRidgeRegression n√™n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p RidgeCV &  BayesianRidgeRegression l√†m 2 l·∫ßn submit cu·ªëi c√πng c·ªßa t√¥i v·ªõi ƒëi·ªÉm s·ªë c≈© l√†   0,447.\n"
      ],
      "id": "6hQu3HXLXrtd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsIaJ45aYmJ9"
      },
      "source": [
        "### Th·ª±c nghi·ªám\n",
        "\n",
        "* Hu·∫•n luy·ªán v·ªõi 4 m√¥ h√¨nh (deberta, roberta base, roberta-squad2 , distilroberta).\n",
        "* M·ªói m√¥ h√¨nh hu·∫•n luy·ªán v·ªõi 5 fold kh√°c nhau.\n",
        "* S·ª≠ d·ª•ng RigdeCV ƒë·ªÉ t·ªïng h·ª£p."
      ],
      "id": "dsIaJ45aYmJ9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ36yrMAvLjL"
      },
      "source": [
        "#### Th∆∞ vi·ªán"
      ],
      "id": "bZ36yrMAvLjL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRqx4Q845VP4",
        "cellView": "form",
        "outputId": "a9400b14-afcd-47f9-97ba-deb44a45f618"
      },
      "source": [
        "# @title requirements.txt\n",
        "%%writefile requirements.txt\n",
        "\n",
        "torch==1.10.0\n",
        "numpy==1.19.2\n",
        "tqdm==4.49.0\n",
        "transformers==4.10.2\n",
        "textstat\n",
        "pytorch_lightning==1.5.4\n",
        "gspread_dataframe==3.2.1\n",
        "gspread==3.7.0\n",
        "pandas==1.3.4\n",
        "wandb==0.11.2\n",
        "coolname==1.1.0\n",
        "accelerate==0.3.0\n",
        "PyYAML==5.4.1\n",
        "scikit_learn==0.24.2"
      ],
      "id": "aRqx4Q845VP4",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtOAqHMT5-ki",
        "cellView": "form",
        "outputId": "769e8d80-070a-4049-b0d3-750bf6276b02"
      },
      "source": [
        "# @title hyperparams\n",
        "%%writefile commonlitreadabilityprize/hyperparams.yml\n",
        "\n",
        "roberta_base:\n",
        "  # model_name: deepset/roberta-base-squad2\n",
        "  model_name: roberta-base\n",
        "  # model_name: xlm-roberta-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "xlm_roberta_base:\n",
        "  model_name: xlm-roberta-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "bigbird:\n",
        "  model_name: google/bigbird-roberta-large\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "roberta_large:\n",
        "  model_name: roberta-large\n",
        "  # model_name: xlm-roberta-large\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "roberta_large_squad:\n",
        "  model_name: deepset/roberta-large-squad2\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "distilroberta:\n",
        "  model_name: distilroberta-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "albert:\n",
        "  model_name: albert-large-v2\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00003\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  eps: 0.000001\n",
        "  betas: !!python/tuple [0.9, 0.999]\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "  warmup: 100\n",
        "  grad_clip_val: 1.0\n",
        "\n",
        "\n",
        "funnel:\n",
        "  model_name: funnel-transformer/large-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  eps: 0.0001\n",
        "  betas: !!python/tuple [0.9, 0.999]\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "  warmup: 100\n",
        "\n",
        "\n",
        "bert_base:\n",
        "  model_name: bert-base-uncased\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "bert_large:\n",
        "  model_name: bert-large-cased\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "bert_large_wm:\n",
        "  model_name: bert-large-cased-whole-word-masking\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "deberta_base:\n",
        "  model_name: microsoft/deberta-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "deberta_large:\n",
        "  model_name: microsoft/deberta-large\n",
        "  gpus: 1\n",
        "  batch_size: 12\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "  \n",
        "deberta_xlarge:\n",
        "  model_name: microsoft/deberta-v2-xlarge\n",
        "  gpus: 1\n",
        "  plugins: ddp_sharded\n",
        "  accelerator: ddp\n",
        "  precision: 16\n",
        "  batch_size: 10\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "bart_base:\n",
        "  model_name: facebook/bart-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "bart_large:\n",
        "  model_name: facebook/bart-large\n",
        "  gpus: 1\n",
        "  batch_size: 12\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "st_labse:\n",
        "  model_name: sentence-transformers/LaBSE\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "# https://www.sbert.net/docs/pretrained_models.html\n",
        "st_paraphrase:\n",
        "  model_name: sentence-transformers/paraphrase-mpnet-base-v2\n",
        "  gpus: 1\n",
        "  batch_size: 32\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "sentence_bert:\n",
        "  model_name: deepset/sentence_bert\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.000025\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "gpt2:\n",
        "  model_name: gpt2\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "electra:\n",
        "  model_name: google/electra-large-discriminator\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00001\n",
        "  warmup: 100\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: True\n",
        "  swa: True\n",
        "\n",
        "\n",
        "convbert:\n",
        "  model_name: YituTech/conv-bert-base\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True\n",
        "\n",
        "\n",
        "ctrl:\n",
        "  model_name: ctrl  # 6.5GB!!\n",
        "  gpus: 1\n",
        "  batch_size: 16\n",
        "  max_epochs: 6\n",
        "  lr: 0.00005\n",
        "  weight_decay: 1.0\n",
        "  val_check_interval: 5\n",
        "  kl_loss: False\n",
        "  swa: True"
      ],
      "id": "CtOAqHMT5-ki",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing commonlitreadabilityprize/hyperparams.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF6VtRsr1uU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65e383b5-d221-42ce-dfc3-a71b19dd8c40"
      },
      "source": [
        "!pip install -qq ruamel.yaml==0.17.4\n",
        "!pip install -r requirements.txt"
      ],
      "id": "dF6VtRsr1uU0",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.19.2)\n",
            "Requirement already satisfied: tqdm==4.49.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.49.0)\n",
            "Collecting transformers==4.10.2\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.8 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: pytorch_lightning==1.5.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: gspread_dataframe==3.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.2.1)\n",
            "Requirement already satisfied: gspread==3.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.7.0)\n",
            "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.3.4)\n",
            "Requirement already satisfied: wandb==0.11.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (0.11.2)\n",
            "Requirement already satisfied: coolname==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: accelerate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (0.3.0)\n",
            "Requirement already satisfied: PyYAML==5.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (5.4.1)\n",
            "Requirement already satisfied: scikit_learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (0.24.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (2.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (0.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.2->-r requirements.txt (line 5)) (4.8.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.18.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (2021.11.1)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.3.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (2.7.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from gspread_dataframe==3.2.1->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from gspread==3.7.0->-r requirements.txt (line 9)) (0.4.6)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from gspread==3.7.0->-r requirements.txt (line 9)) (1.35.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r requirements.txt (line 10)) (2018.9)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (5.2.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (3.1.24)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (1.0.8)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (1.26.7)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (2.3)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (3.5.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (1.5.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (3.17.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.2->-r requirements.txt (line 11)) (5.4.8)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate==0.3.0->-r requirements.txt (line 13)) (21.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2->-r requirements.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2->-r requirements.txt (line 15)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==0.24.2->-r requirements.txt (line 15)) (1.4.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb==0.11.2->-r requirements.txt (line 11)) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.11.2->-r requirements.txt (line 11)) (5.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread==3.7.0->-r requirements.txt (line 9)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread==3.7.0->-r requirements.txt (line 9)) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread==3.7.0->-r requirements.txt (line 9)) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.12.0->gspread==3.7.0->-r requirements.txt (line 9)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib>=0.4.1->gspread==3.7.0->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.2->-r requirements.txt (line 5)) (3.0.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread==3.7.0->-r requirements.txt (line 9)) (0.4.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.2->-r requirements.txt (line 5)) (2.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.2->-r requirements.txt (line 5)) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.2->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread==3.7.0->-r requirements.txt (line 9)) (3.1.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (1.42.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.2->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (4.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.5.4->-r requirements.txt (line 7)) (1.7.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.8.2\n",
            "    Uninstalling transformers-4.8.2:\n",
            "      Successfully uninstalled transformers-4.8.2\n",
            "Successfully installed transformers-4.10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nsqA1uEsVbt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import yaml\n",
        "from ruamel.yaml.comments import  CommentedMap as OrderedDict, CommentedSeq as OrderedList\n",
        "from ruamel.yaml.main import round_trip_load as yaml_load, round_trip_dump as yaml_dump\n",
        "from ruamel.yaml import YAML"
      ],
      "id": "7nsqA1uEsVbt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "134f7Bx0N8lX",
        "outputId": "74c96702-f4ce-4998-d846-cac4cb322f01"
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "id": "134f7Bx0N8lX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCmBE-eKwJNX",
        "outputId": "1bde4c64-4bd7-4ce8-d37a-ce8eb2148998"
      },
      "source": [
        "pip install torchtext==0.11.0"
      ],
      "id": "JCmBE-eKwJNX",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.26.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.19.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.49.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.0.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.26.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuJuclwiwgEZ"
      },
      "source": [
        "config = {\"roberta_base\":\n",
        "          {\"model_name\": \"deepset/roberta-base-squad2\",\n",
        "           \"gpus\":1,\n",
        "           \"batch_size\":16,\n",
        "           \"max_epochs\":6,\n",
        "           \"lr\":\"0.00005\",\n",
        "           \"weight_decay\": 0.1,\n",
        "           \"val_check_interval\":5,\n",
        "           \"kl_loss\": True,\n",
        "           \"swa\": True\n",
        "          }} \n",
        "\n",
        "yml = YAML()\n",
        "\n",
        "# print(yaml_dump(shopping_list))\n",
        "with open(r'hyperparams.yml', 'wb') as file:\n",
        "    documents = yml.dump(config, file)"
      ],
      "id": "ZuJuclwiwgEZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD9PyELAAVBL"
      },
      "source": [
        "#### Utils"
      ],
      "id": "ZD9PyELAAVBL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg0fODWbAJlu"
      },
      "source": [
        "def add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if len(param.shape) == 1 or any(s in name for s in skip_list):\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "    return [\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_optimizer_params(model, type=\"s\"):\n",
        "    # differential learning rate and weight decay\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    learning_rate = 5e-5\n",
        "    no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "    if type == \"s\":\n",
        "        optimizer_parameters = filter(lambda x: x.requires_grad, model.parameters())\n",
        "    elif type == \"i\":\n",
        "\n",
        "        optimizer_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters() if \"transformer\" not in n\n",
        "                ],\n",
        "                \"lr\": 1e-3,\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "            },\n",
        "        ]\n",
        "    elif type == \"a\":\n",
        "        group1 = [\"layer.0.\", \"layer.1.\", \"layer.2.\", \"layer.3.\"]\n",
        "        group2 = [\"layer.4.\", \"layer.5.\", \"layer.6.\", \"layer.7.\"]\n",
        "        group3 = [\"layer.8.\", \"layer.9.\", \"layer.10.\", \"layer.11.\"]\n",
        "        group_all = [\n",
        "            \"layer.0.\",\n",
        "            \"layer.1.\",\n",
        "            \"layer.2.\",\n",
        "            \"layer.3.\",\n",
        "            \"layer.4.\",\n",
        "            \"layer.5.\",\n",
        "            \"layer.6.\",\n",
        "            \"layer.7.\",\n",
        "            \"layer.8.\",\n",
        "            \"layer.9.\",\n",
        "            \"layer.10.\",\n",
        "            \"layer.11.\",\n",
        "        ]\n",
        "        optimizer_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                    and not any(nd in n for nd in group_all)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                    and any(nd in n for nd in group1)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "                \"lr\": learning_rate / 2.6,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                    and any(nd in n for nd in group2)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "                \"lr\": learning_rate,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if not any(nd in n for nd in no_decay)\n",
        "                    and any(nd in n for nd in group3)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,\n",
        "                \"lr\": learning_rate * 2.6,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay)\n",
        "                    and not any(nd in n for nd in group_all)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,\n",
        "                \"lr\": learning_rate / 2.6,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,\n",
        "                \"lr\": learning_rate,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p\n",
        "                    for n, p in model.transformer.named_parameters()\n",
        "                    if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,\n",
        "                \"lr\": learning_rate * 2.6,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in model.named_parameters() if \"transformer\" not in n\n",
        "                ],\n",
        "                \"lr\": 1e-3,\n",
        "                # \"momentum\": 0.99,\n",
        "            },\n",
        "        ]\n",
        "    return optimizer_parameters"
      ],
      "id": "Fg0fODWbAJlu",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQlF4Y-G-1eT"
      },
      "source": [
        "#### CommonLitModel"
      ],
      "id": "cQlF4Y-G-1eT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3fGHPDR-tYy"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from transformers import AutoConfig, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
        "from pathlib import Path\n",
        "\n",
        "COMP_NAME = \"commonlitreadabilityprize\"\n",
        "\n",
        "INPUT_PATH = Path(f\"input_data/{COMP_NAME}/\")\n",
        "OUTPUT_PATH = Path(f\"output/{COMP_NAME}/\")\n",
        "CONFIG_PATH = Path(f\"{COMP_NAME}/hyperparams.yml\")\n",
        "MODEL_CACHE = Path(\"model_cache/\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_features, middle_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = middle_features\n",
        "        self.out_features = out_features\n",
        "        self.W = nn.Linear(in_features, middle_features)\n",
        "        self.V = nn.Linear(middle_features, out_features)\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        return context_vector\n",
        "\n",
        "\n",
        "class CommonLitModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"roberta-base\",\n",
        "        lr: float = 0.001,\n",
        "        weight_decay: float = 0,\n",
        "        pretrained: bool = False,\n",
        "        betas: tuple = (0.9, 0.999),\n",
        "        eps: float = 1e-6,\n",
        "        kl_loss: bool = False,\n",
        "        warmup: int = 100,\n",
        "        hf_config=None,\n",
        "        pooled: bool = False,\n",
        "        use_hidden: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        if hf_config is None:\n",
        "            if pretrained:\n",
        "                model_path = OUTPUT_PATH / \"pretraining\" / model_name\n",
        "                print(\"Using pretrained from\", model_path)\n",
        "                self.config = AutoConfig.from_pretrained(model_name)\n",
        "                self.transformer = AutoModel.from_pretrained(\n",
        "                    model_path, output_hidden_states=True\n",
        "                )\n",
        "            else:\n",
        "                self.config = AutoConfig.from_pretrained(\n",
        "                    model_name,\n",
        "                    cache_dir=MODEL_CACHE / model_name,\n",
        "                )\n",
        "                self.transformer = AutoModel.from_pretrained(\n",
        "                    model_name,\n",
        "                    cache_dir=MODEL_CACHE / model_name,\n",
        "                    output_hidden_states=True,\n",
        "                )\n",
        "        else:\n",
        "            self.config = hf_config\n",
        "            self.config.output_hidden_states = True\n",
        "            self.transformer = AutoModel.from_config(hf_config)\n",
        "\n",
        "        # self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
        "        # Multi sample Dropout\n",
        "        # self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
        "        # self.regressor = nn.Linear(self.config.hidden_size, 2)\n",
        "\n",
        "        if use_hidden:\n",
        "            n_hidden = self.config.hidden_size * 2\n",
        "        else:\n",
        "            n_hidden = self.config.hidden_size\n",
        "\n",
        "        self.seq_attn_head = nn.Sequential(\n",
        "            nn.LayerNorm(n_hidden),\n",
        "            # nn.Dropout(0.1),\n",
        "            AttentionBlock(n_hidden, n_hidden, 1),\n",
        "            # nn.Dropout(0.1),\n",
        "            # nn.Linear(self.config.hidden_size, 2 if kl_loss else 1),\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Linear(n_hidden + 2, 2 if kl_loss else 1)\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        # out = self.transformer(**kwargs)[\"logits\"]\n",
        "\n",
        "        model_out = self.transformer(**kwargs)  # 0=seq_output, 1=pooler_output\n",
        "        # x = self.layer_norm(x)\n",
        "        # for i, dropout in enumerate(self.dropouts):\n",
        "        #     if i == 0:\n",
        "        #         out = self.regressor(dropout(x))\n",
        "        #     else:\n",
        "        #         out += self.regressor(dropout(x))\n",
        "        # out /= len(self.dropouts)\n",
        "\n",
        "        if self.hparams.use_hidden:\n",
        "            states = model_out[2]\n",
        "            out = torch.stack(\n",
        "                tuple(states[-i - 1] for i in range(self.config.num_hidden_layers)),\n",
        "                dim=0,\n",
        "            )\n",
        "            out_mean = torch.mean(out, dim=0)\n",
        "            out_max, _ = torch.max(out, dim=0)\n",
        "            out = torch.cat((out_mean, out_max), dim=-1)\n",
        "        else:\n",
        "            out = model_out[0]\n",
        "\n",
        "        out = self.seq_attn_head(out)\n",
        "        out = torch.cat([out, features], -1)\n",
        "        out = self.regressor(out)\n",
        "\n",
        "        if out.shape[1] == 1:\n",
        "            return out, None\n",
        "        else:\n",
        "            mean = out[:, 0].view(-1, 1)\n",
        "            log_var = out[:, 1].view(-1, 1)\n",
        "            return mean, log_var\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels, features = batch\n",
        "        mean, log_var = self.forward(features, **inputs)\n",
        "        if self.hparams.kl_loss:\n",
        "            p = torch.distributions.Normal(mean, torch.exp(log_var))\n",
        "            q = torch.distributions.Normal(labels[\"target\"], labels[\"error\"])\n",
        "            loss = torch.distributions.kl_divergence(p, q).mean()\n",
        "        else:\n",
        "            loss = self.loss_fn(mean, labels[\"target\"])\n",
        "        self.log_dict({\"loss/train_step\": loss})\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in training_step_outputs]).mean()\n",
        "        self.log(\"loss/train\", avg_loss, sync_dist=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels, features = batch\n",
        "        mean, log_var = self.forward(features, **inputs)\n",
        "        if self.hparams.kl_loss:\n",
        "            p = torch.distributions.Normal(mean, torch.exp(log_var))\n",
        "            q = torch.distributions.Normal(labels[\"target\"], labels[\"error\"])\n",
        "            loss = torch.distributions.kl_divergence(p, q).mean()\n",
        "        else:\n",
        "            loss = self.loss_fn(mean, labels[\"target\"])\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": loss,\n",
        "            \"y_pred\": mean,\n",
        "            \"y_true\": labels[\"target\"],\n",
        "        }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss_val = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        y_pred = torch.cat([x[\"y_pred\"] for x in outputs])\n",
        "        y_true = torch.cat([x[\"y_true\"] for x in outputs])\n",
        "\n",
        "        rmse = torch.sqrt(self.loss_fn(y_pred, y_true))\n",
        "\n",
        "        self.log_dict(\n",
        "            {\n",
        "                \"loss/valid\": loss_val,\n",
        "                \"rmse\": rmse,\n",
        "            },\n",
        "            prog_bar=True,\n",
        "            sync_dist=True,\n",
        "        )\n",
        "\n",
        "    # learning rate warm-up\n",
        "    def optimizer_step(\n",
        "        self,\n",
        "        epoch,\n",
        "        batch_idx,\n",
        "        optimizer,\n",
        "        optimizer_idx,\n",
        "        optimizer_closure,\n",
        "        on_tpu=False,\n",
        "        using_native_amp=False,\n",
        "        using_lbfgs=False,\n",
        "    ):\n",
        "        # Warm-up the first 100 steps\n",
        "        if self.trainer.global_step < self.hparams.warmup:\n",
        "            lr_scale = min(\n",
        "                1.0, float(self.trainer.global_step + 1) / self.hparams.warmup\n",
        "            )\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = lr_scale * self.hparams.lr\n",
        "\n",
        "        # update params\n",
        "        optimizer.step(closure=optimizer_closure)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        parameters = add_weight_decay(\n",
        "            self,\n",
        "            self.hparams.weight_decay,\n",
        "            skip_list=[\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"],\n",
        "        )\n",
        "\n",
        "        # parameters = get_optimizer_params(self, \"a\")\n",
        "\n",
        "        # parameters = [\n",
        "        #     {\n",
        "        #         \"params\": self.transformer.parameters(),\n",
        "        #         \"weight_decay\": 0,\n",
        "        #         \"lr\": self.hparams.lr,\n",
        "        #     },\n",
        "        #     {\n",
        "        #         \"params\": [\n",
        "        #             p for n, p in self.named_parameters() if \"transformer\" not in n\n",
        "        #         ],\n",
        "        #         \"weight_decay\": self.hparams.weight_decay,\n",
        "        #         \"lr\": 1e-3,\n",
        "        #     },\n",
        "        # ]\n",
        "\n",
        "        opt = AdamW(\n",
        "            parameters,\n",
        "            lr=self.hparams.lr,\n",
        "            betas=self.hparams.betas,\n",
        "            eps=self.hparams.eps,\n",
        "        )\n",
        "\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            opt, T_max=1000, eta_min=self.hparams.lr / 10\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": opt,\n",
        "            \"lr_scheduler\": {\"scheduler\": sch, \"interval\": \"step\"},\n",
        "        }"
      ],
      "id": "e3fGHPDR-tYy",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx77E83MBvzn"
      },
      "source": [
        "#### Dataset"
      ],
      "id": "lx77E83MBvzn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_WERf6dBttx"
      },
      "source": [
        "from logging import error\n",
        "from numpy.core.fromnumeric import std\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textstat\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "# https://www.kaggle.com/abhishek/step-1-create-folds\n",
        "def create_folds(data, n_splits, random_state=None):\n",
        "    # we create a new column called fold and fill it with -1\n",
        "    data[\"fold\"] = -1\n",
        "\n",
        "    # the next step is to randomize the rows of the data\n",
        "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    # calculate number of bins by Sturge's rule\n",
        "    # I take the floor of the value, you can also\n",
        "    # just round it\n",
        "    num_bins = int(np.floor(1 + np.log2(len(data))))\n",
        "\n",
        "    # bin targets\n",
        "    data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=num_bins, labels=False)\n",
        "\n",
        "    # initiate the kfold class from model_selection module\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    # fill the new kfold column\n",
        "    # note that, instead of targets, we use bins!\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, \"fold\"] = f\n",
        "\n",
        "    # drop the bins column\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "\n",
        "    # return dataframe with folds\n",
        "    return data\n",
        "\n",
        "\n",
        "class CommonLitDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=256):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.excerpt = self.df[\"excerpt\"]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "        inputs = self.tokenizer(\n",
        "            str(row[\"excerpt\"]),\n",
        "            max_length=self.max_len,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            add_special_tokens=True  # not sure what this does\n",
        "        )\n",
        "\n",
        "        input_dict = {\n",
        "            \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        if \"target\" in self.df.columns:\n",
        "            labels = {\n",
        "                \"target\": torch.tensor([row[\"target\"]], dtype=torch.float32),\n",
        "                \"error\": torch.tensor([row[\"standard_error\"]], dtype=torch.float32),\n",
        "            }\n",
        "\n",
        "            # For id 436ce79fe\n",
        "            if labels[\"error\"] <= 0:\n",
        "                labels[\"error\"] += 0.5\n",
        "\n",
        "            labels[\"target_stoch\"] = torch.normal(\n",
        "                mean=labels[\"target\"], std=labels[\"error\"]\n",
        "            )\n",
        "        else:\n",
        "            labels = 0\n",
        "\n",
        "        # Add addtional features\n",
        "        features = self.generate_features(str(row[\"excerpt\"]))\n",
        "\n",
        "        return input_dict, labels, features\n",
        "\n",
        "    def generate_features(self, text):\n",
        "        means = torch.tensor([67.742121, 10.308363])\n",
        "        stds = torch.tensor([17.530230, 3.298237])\n",
        "        features = torch.tensor(\n",
        "            [\n",
        "                # textstat.sentence_count(text),\n",
        "                # textstat.lexicon_count(text),\n",
        "                textstat.flesch_reading_ease(text),\n",
        "                textstat.smog_index(text),\n",
        "            ]\n",
        "        )\n",
        "        return (features - means) / stds\n",
        "\n",
        "\n",
        "class CommonLitDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size: int = 32,\n",
        "        model_name: str = \"roberta-base\",\n",
        "        max_len: int = 256,\n",
        "        seed: int = 48,\n",
        "        folds: int = 5,\n",
        "        num_workers: int = 16,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            cache_dir=MODEL_CACHE / model_name,\n",
        "        )\n",
        "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        self.batch_size = batch_size\n",
        "        self.max_len = max_len\n",
        "        self.num_workers = num_workers\n",
        "        self.df = pd.read_csv(INPUT_PATH / \"train.csv\")\n",
        "        self.df = create_folds(self.df, folds, seed)\n",
        "\n",
        "    def setup(self, stage=None, fold_n: int = 0):\n",
        "        trn_df = self.df.query(f\"fold != {fold_n}\")\n",
        "        val_df = self.df.query(f\"fold == {fold_n}\")\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.clr_train = CommonLitDataset(trn_df, self.tokenizer, self.max_len)\n",
        "            self.clr_valid = CommonLitDataset(val_df, self.tokenizer, self.max_len)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.clr_train,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.clr_valid,\n",
        "            batch_size=128,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "        )"
      ],
      "id": "-_WERf6dBttx",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmeBThJcCvSZ"
      },
      "source": [
        "#### Training "
      ],
      "id": "dmeBThJcCvSZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Gp8Ae-Tcvy"
      },
      "source": [
        "torch.hub.list(\"pytorch/vision\")"
      ],
      "id": "N4Gp8Ae-Tcvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98GQ3mdcY7ii"
      },
      "source": [
        "##### Config"
      ],
      "id": "98GQ3mdcY7ii"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4aaLOKBUK3Y"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "from argparse import ArgumentParser\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import yaml\n",
        "from coolname import generate_slug\n",
        "from pytorch_lightning.callbacks import (\n",
        "    EarlyStopping,\n",
        "    LearningRateMonitor,\n",
        "    ModelCheckpoint,\n",
        ")\n",
        "from pytorch_lightning.loggers import NeptuneLogger, TensorBoardLogger, WandbLogger\n",
        "\n",
        "\n",
        "\n",
        "def prepare_args(config_path=CONFIG_PATH, default_config=\"deberta_base\"):\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--config\",\n",
        "        action=\"store\",\n",
        "        dest=\"config\",\n",
        "        help=\"Configuration scheme\",\n",
        "        default=default_config,\n",
        "    )\n",
        "\n",
        "    # parser.add_argument(\n",
        "    #     \"--gpus\",\n",
        "    #     action=\"store\",\n",
        "    #     dest=\"gpus\",\n",
        "    #     help=\"Number of GPUs\",\n",
        "    #     default=2,\n",
        "    #     type=int,\n",
        "    # )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--timestamp\",\n",
        "        action=\"store\",\n",
        "        dest=\"timestamp\",\n",
        "        help=\"Timestamp for versioning\",\n",
        "        default=str(datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
        "        type=str,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fold\",\n",
        "        action=\"store\",\n",
        "        dest=\"fold\",\n",
        "        help=\"Fold number\",\n",
        "        default=1,\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--seed\",\n",
        "        action=\"store\",\n",
        "        dest=\"seed\",\n",
        "        help=\"Random seed\",\n",
        "        default=48,\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--slug\",\n",
        "        action=\"store\",\n",
        "        dest=\"slug\",\n",
        "        help=\"Human rememebrable run group\",\n",
        "        default=generate_slug(3),\n",
        "        type=str,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--logging\",\n",
        "        dest=\"logging\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Flag to log to WandB (on by default)\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--no-logging\",\n",
        "        dest=\"logging\",\n",
        "        action=\"store_false\",\n",
        "        help=\"Flag to prevent logging\",\n",
        "    )\n",
        "    parser.set_defaults(logging=True)\n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "\n",
        "    # Lookup the config from the YAML file and set args\n",
        "    with open(config_path, \"r\") as ymlfile:\n",
        "        cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
        "\n",
        "        if args.config != default_config:\n",
        "            print(\"Using\", args.config, \"configuration\")\n",
        "\n",
        "        for k, v in cfg[args.config].items():\n",
        "            setattr(args, k, v)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def resume_helper(args):\n",
        "    \"\"\"\n",
        "    To resume a run, add this to the YAML/args:\n",
        "    checkpoint: \"20210510-161949\"\n",
        "    wandb_id: 3j79kxq6\n",
        "    Args:\n",
        "        args ([type]): [description]\n",
        "    Returns:\n",
        "        [type]: [description]\n",
        "    \"\"\"\n",
        "    if hasattr(args, \"checkpoint\"):\n",
        "        paths = (\n",
        "            OUTPUT_PATH / args.checkpoint / args.encoder / f\"fold_{args.fold - 1}\"\n",
        "        ).glob(\"*.*loss.ckpt\")\n",
        "        resume = list(paths)[0]\n",
        "\n",
        "        if hasattr(args, \"wandb_id\"):\n",
        "            run_id = args.wandb_id\n",
        "        else:\n",
        "            print(\"No wandb_id provided. Logging as new run\")\n",
        "            run_id = None\n",
        "    else:\n",
        "        resume = None\n",
        "        run_id = None\n",
        "\n",
        "    return resume, run_id\n",
        "\n",
        "\n",
        "def prepare_loggers_and_callbacks(\n",
        "    timestamp,\n",
        "    encoder_name,\n",
        "    fold,\n",
        "    monitors=[],\n",
        "    patience=None,\n",
        "    tensorboard=False,\n",
        "    wandb=False,\n",
        "    neptune=False,\n",
        "    run_id=None,\n",
        "    save_weights_only=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Utility function to prepare loggers and callbacks\n",
        "    Args:\n",
        "        timestamp (str): Timestamp for folder name\n",
        "        encoder_name (str): encoder_name for folder name\n",
        "        fold (int): Fold number for folder nesting\n",
        "        monitors (list, optional): For multiple monitors for ModelCheckpoint.\n",
        "        patience (int, optional): patience for EarlyStopping\n",
        "        List of tuples in form [(monitor, mode, suffix), ...],\n",
        "        Defaults to [].\n",
        "        tensorboard (bool): Flag to use Tensorboard logger\n",
        "        wandb (bool): Flag to use Weight and Biases logger\n",
        "        neptune (bool): Flag to use Neptune logger\n",
        "    Returns:\n",
        "        [type]: [description]\n",
        "    \"\"\"\n",
        "    save_path = OUTPUT_PATH / timestamp\n",
        "\n",
        "    callbacks = [LearningRateMonitor(logging_interval=\"epoch\")]\n",
        "\n",
        "    if \"/\" in encoder_name:\n",
        "        encoder_name = encoder_name.replace(\"/\", \"_\")\n",
        "\n",
        "    if patience:\n",
        "        callbacks.append(EarlyStopping(\"loss/valid\", patience=patience))\n",
        "\n",
        "    for monitor, mode, suffix in monitors:\n",
        "\n",
        "        if suffix is not None and suffix != \"\":\n",
        "            filename = \"{epoch:02d}-{rmse:.4f}\" + f\"_{suffix}\"\n",
        "        else:\n",
        "            filename = \"{epoch:02d}-{rmse:.4f}\"\n",
        "\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            dirpath=save_path / encoder_name / f\"fold_{fold}\",\n",
        "            filename=filename,\n",
        "            monitor=monitor,\n",
        "            mode=mode,\n",
        "            save_weights_only=save_weights_only,\n",
        "        )\n",
        "        callbacks.append(checkpoint)\n",
        "\n",
        "    loggers = []\n",
        "\n",
        "    if tensorboard:\n",
        "        tb_logger = TensorBoardLogger(\n",
        "            save_dir=save_path,\n",
        "            name=encoder_name,\n",
        "            version=f\"fold_{fold}\",\n",
        "        )\n",
        "        loggers.append(tb_logger)\n",
        "\n",
        "    if wandb:\n",
        "        wandb_logger = WandbLogger(\n",
        "            name=f\"{timestamp}/fold{fold}\",\n",
        "            save_dir=OUTPUT_PATH,\n",
        "            project=COMP_NAME,\n",
        "            id=run_id,\n",
        "        )\n",
        "        loggers.append(wandb_logger)\n",
        "\n",
        "    if neptune:\n",
        "        neptune_logger = NeptuneLogger(\n",
        "            api_key=os.environ[\"NEPTUNE_API_TOKEN\"],\n",
        "            project_name=f\"anjum48/{COMP_NAME}\",\n",
        "            experiment_name=f\"{timestamp}-fold{fold}\",\n",
        "        )\n",
        "        loggers.append(neptune_logger)\n",
        "\n",
        "    return loggers, callbacks\n",
        "\n",
        "\n",
        "def memory_cleanup():\n",
        "    \"\"\"\n",
        "    Cleans up GPU memory. Call after a fold is trained.\n",
        "    https://github.com/huggingface/transformers/issues/1742\n",
        "    \"\"\"\n",
        "    for obj in gc.get_objects():\n",
        "        if torch.is_tensor(obj):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size, requires_grad=False).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_data_multiobjective(x, y1, y2, alpha=1.0):\n",
        "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size, requires_grad=False).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y1_a, y1_b = y1, y1[index]\n",
        "    y2_a, y2_b = y2, y2[index]\n",
        "    return mixed_x, y1_a, y1_b, y2_a, y2_b, lam\n",
        "\n",
        "\n",
        "# https://github.com/clovaai/CutMix-PyTorch/blob/2d8eb68faff7fe4962776ad51d175c3b01a25734/train.py#L227-L238\n",
        "# https://arxiv.org/pdf/1905.04899.pdf\n",
        "def cutmix_data(x, y, alpha):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    rand_index = torch.randperm(x.shape[0]).to(x.device)\n",
        "    y_a = y\n",
        "    y_b = y[rand_index]\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.shape, lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "    # adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.shape[-1] * x.shape[-2]))\n",
        "    return x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1.0 - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "# https://github.com/pytorch/pytorch/issues/21987#issuecomment-539402619\n",
        "def nanmean(v, *args, inplace=False, **kwargs):\n",
        "    if not inplace:\n",
        "        v = v.clone()\n",
        "    is_nan = torch.isnan(v)\n",
        "    is_inf = torch.isinf(v)\n",
        "    v[is_nan] = 0\n",
        "    v[is_inf] = 0\n",
        "    return v.sum(*args, **kwargs) / (~is_nan).float().sum(*args, **kwargs)\n",
        "\n",
        "\n",
        "def nanstd(v, *args, inplace=False, unbiased=True, **kwargs):\n",
        "    if not inplace:\n",
        "        v = v.clone()\n",
        "    is_nan = torch.isnan(v)\n",
        "    is_inf = torch.isinf(v)\n",
        "    v[is_nan] = 0\n",
        "    v[is_inf] = 0\n",
        "\n",
        "    mean = nanmean(v, *args, inplace=False, **kwargs)\n",
        "    numerator = ((v - mean) ** 2).sum(*args, **kwargs)\n",
        "    N = (~is_nan).float().sum(*args, **kwargs)\n",
        "\n",
        "    if unbiased:\n",
        "        N -= 1\n",
        "\n",
        "    return torch.sqrt(numerator / N)\n",
        "\n",
        "\n",
        "def nanstd_mean(v, *args, inplace=False, unbiased=True, **kwargs):\n",
        "    if not inplace:\n",
        "        v = v.clone()\n",
        "    is_nan = torch.isnan(v)\n",
        "    is_inf = torch.isinf(v)\n",
        "    v[is_nan] = 0\n",
        "    v[is_inf] = 0\n",
        "\n",
        "    mean = v.sum(*args, **kwargs) / (~is_nan).float().sum(*args, **kwargs)\n",
        "    numerator = ((v - mean) ** 2).sum(*args, **kwargs)\n",
        "    N = (~is_nan).float().sum(*args, **kwargs)\n",
        "\n",
        "    if unbiased:\n",
        "        N -= 1\n",
        "\n",
        "    std = torch.sqrt(numerator / N)\n",
        "    return std, mean\n"
      ],
      "id": "_4aaLOKBUK3Y",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBqhVxUjYzJl"
      },
      "source": [
        "##### Main"
      ],
      "id": "tBqhVxUjYzJl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r71pRVMoCk7f"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
        "from pytorch_lightning.plugins import DDPPlugin\n",
        "\n",
        "\n",
        "torch.hub.set_dir(MODEL_CACHE)\n",
        "\n",
        "\n",
        "def run_fold(fold: int, args):\n",
        "    pl.seed_everything(args.seed + fold)\n",
        "    resume, run_id = resume_helper(args)\n",
        "\n",
        "    monitor_list = [(\"rmse\", \"min\", None)]\n",
        "    loggers, callbacks = prepare_loggers_and_callbacks(\n",
        "        args.timestamp,\n",
        "        args.model_name,\n",
        "        fold,\n",
        "        monitors=monitor_list,\n",
        "        tensorboard=args.logging,\n",
        "        wandb=args.logging,\n",
        "        patience=None,\n",
        "        run_id=run_id,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    if args.swa:\n",
        "        swa = StochasticWeightAveraging(swa_epoch_start=0.5)\n",
        "        callbacks.append(swa)\n",
        "\n",
        "    model = CommonLitModel(**args.__dict__)\n",
        "\n",
        "    trainer = pl.Trainer().from_argparse_args(\n",
        "        args,\n",
        "        logger=loggers,\n",
        "        callbacks=callbacks,\n",
        "        # plugins=DDPPlugin(find_unused_parameters=False),\n",
        "        resume_from_checkpoint=resume,\n",
        "        # fast_dev_run=True,\n",
        "        # auto_lr_find=True,\n",
        "    )\n",
        "\n",
        "    dm = CommonLitDataModule().from_argparse_args(args)\n",
        "    dm.setup(\"fit\", fold)\n",
        "\n",
        "    # Save tokenizer\n",
        "    folder = args.model_name\n",
        "    if \"/\" in folder:\n",
        "        folder = folder.replace(\"/\", \"_\")\n",
        "\n",
        "    save_path = OUTPUT_PATH / args.timestamp / folder / f\"fold_{fold}\"\n",
        "    dm.tokenizer.save_pretrained(save_path)\n",
        "    model.config.to_json_file(str(save_path / \"config.json\"))\n",
        "\n",
        "    # trainer.tune(model, datamodule=dm)  # Use with auto_lr_find\n",
        "    trainer.fit(model, datamodule=dm)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = prepare_args(default_config = \"gpt2\")\n",
        "    args.logging = False\n",
        "    args.fold = 1\n",
        "    # args.config = \"deberta_base\"\n",
        "    args.seed = 100\n",
        "    run_fold(args.fold - 1,args)"
      ],
      "id": "r71pRVMoCk7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFpXMuGZrcwO"
      },
      "source": [
        "memory_cleanup()"
      ],
      "id": "KFpXMuGZrcwO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuuB_BEf-u4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6aac1f-11a1-42f9-bde7-eb7f9f42b779"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "XuuB_BEf-u4L",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K1vL74HbtCg"
      },
      "source": [
        "!cp -R \"/content/drive/MyDrive/CommonLit Readability Prize/model.tar.gz\" . "
      ],
      "id": "2K1vL74HbtCg",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTTA5V5DcFrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f2d5dc-8fee-481d-fba0-255567f0ea53"
      },
      "source": [
        "!tar -xvf model.tar.gz"
      ],
      "id": "nTTA5V5DcFrp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commonlitreadabilityprize/\n",
            "commonlitreadabilityprize/hyperparams.yml\n",
            "input_data/\n",
            "input_data/commonlitreadabilityprize/\n",
            "input_data/commonlitreadabilityprize/sample_submission.csv\n",
            "input_data/commonlitreadabilityprize/test.csv\n",
            "input_data/commonlitreadabilityprize/train.csv\n",
            "output/\n",
            "output/commonlitreadabilityprize/\n",
            "output/commonlitreadabilityprize/20211205-134729/\n",
            "output/commonlitreadabilityprize/20211205-134729/distil_oofs_0.45617.csv\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/epoch=01-rmse=0.4787.ckpt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/epoch=01-rmse=0.5101.ckpt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/epoch=01-rmse=0.5196.ckpt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/epoch=04-rmse=0.5199.ckpt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/epoch=02-rmse=0.5231.ckpt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/vocab.json\n",
            "output/commonlitreadabilityprize/20211204-023300/\n",
            "output/commonlitreadabilityprize/20211204-023300/oofs_1.28072.csv\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/epoch=02-rmse=0.4815.ckpt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/merges.txt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_1/vocab.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/merges.txt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/epoch=03-rmse=0.5111.ckpt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_4/vocab.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/epoch=02-rmse=0.4804.ckpt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/merges.txt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_3/vocab.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/merges.txt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/epoch=02-rmse=0.4774.ckpt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_2/vocab.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/epoch=03-rmse=0.5036.ckpt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/merges.txt\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211204-023300/deepset_roberta-base-squad2/fold_0/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-062042/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/.DS_Store\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/epoch=01-rmse=0.5007.ckpt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_1/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/epoch=01-rmse=0.5126.ckpt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_4/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/epoch=02-rmse=0.4985.ckpt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_3/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/epoch=03-rmse=0.5016.ckpt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_2/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/epoch=05-rmse=0.5072.ckpt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/merges.txt\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211205-062042/microsoft_deberta-base/fold_0/vocab.json\n",
            "output/commonlitreadabilityprize/20211205-062042/deberta_oofs_0.50414.csv\n",
            "output/commonlitreadabilityprize/20211205-062042/.DS_Store\n",
            "output/commonlitreadabilityprize/.ipynb_checkpoints/\n",
            "output/commonlitreadabilityprize/20211206-015956/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/merges.txt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/epoch=03-rmse=0.4978.ckpt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_1/vocab.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/merges.txt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/vocab.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_4/epoch=03-rmse=0.4798.ckpt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/epoch=02-rmse=0.5051.ckpt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/merges.txt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_3/vocab.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/merges.txt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/epoch=02-rmse=0.4932.ckpt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_2/vocab.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/special_tokens_map.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/epoch=02-rmse=0.4856.ckpt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/tokenizer.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/merges.txt\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/tokenizer_config.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta-base/fold_0/vocab.json\n",
            "output/commonlitreadabilityprize/20211206-015956/roberta_base_oofs_0.49240.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvFI3W6b--Pj"
      },
      "source": [
        "!cp -R model.tar.gz \"/content/drive/MyDrive/CommonLit Readability Prize\""
      ],
      "id": "EvFI3W6b--Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-9wi1ZC6Xf_"
      },
      "source": [
        "!tar -czf model.tar.gz commonlitreadabilityprize input_data output"
      ],
      "id": "h-9wi1ZC6Xf_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZrQi-dYDdw"
      },
      "source": [
        "#### Inference"
      ],
      "id": "fSZrQi-dYDdw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fWhm3tiX_MI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9392ff23-52ee-4626-cb7d-a9c92a12d238"
      },
      "source": [
        "import os\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoConfig\n",
        "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def infer(model, dataset, batch_size=64, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for input_dict, _, features in loader:\n",
        "            input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
        "            mean, log_var = model(features.to(device), **input_dict)\n",
        "            predictions.append(mean.cpu())\n",
        "\n",
        "    return torch.cat(predictions, 0)\n",
        "\n",
        "\n",
        "def make_oofs(folder_name, seed, device=\"cuda\"):\n",
        "    mpaths = sorted(list((OUTPUT_PATH / folder_name).glob(f\"*/*/*.ckpt\")))\n",
        "    # mpaths = sorted(list((OUTPUT_PATH ).glob(f\"*/*/*/*.ckpt\")))\n",
        "    print(mpaths)\n",
        "    tokenizers = [AutoTokenizer.from_pretrained(str(p.parent)) for p in mpaths]\n",
        "    configs = [AutoConfig.from_pretrained(str(p.parent)) for p in mpaths]\n",
        "    models = [\n",
        "        CommonLitModel.load_from_checkpoint(p, hf_config=c)\n",
        "        for p, c in zip(mpaths, configs)\n",
        "    ]\n",
        "    print(\n",
        "        f\"{len(mpaths)} models found.\",\n",
        "        f\"{len(tokenizers)} tokenizers found.\",\n",
        "        f\"{len(configs)} configs found\",\n",
        "    )\n",
        "\n",
        "    df = pd.read_csv(INPUT_PATH / \"train.csv\")\n",
        "    df = create_folds(df, 5, seed)\n",
        "    df[\"prediction\"] = 0\n",
        "\n",
        "    for fold, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
        "        print(fold)\n",
        "        df_fold = df.query(f\"fold == {fold}\")\n",
        "        dataset = CommonLitDataset(df_fold, tokenizer)\n",
        "        df.loc[df_fold.index, \"prediction\"] = (\n",
        "            infer(model, dataset, device=device).squeeze().numpy()\n",
        "        )\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(df[\"prediction\"], df[\"target\"]))\n",
        "    print(f\"OOF RMSE {rmse:0.5f}\")\n",
        "    df.to_csv(OUTPUT_PATH / folder_name / f\"oofs_{rmse:0.5f}.csv\", index=False)\n",
        "    # df.to_csv(OUTPUT_PATH  / f\"oofs_{rmse:0.5f}.csv\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    default_checkpoint = \"20211205-134729\" # commonlitreadabilityprize\n",
        "    # 20211206-015956 roberta-base\n",
        "    # 20211204-023300 squad2\n",
        "    # 20211205-134729 distil\n",
        "    # 20211205-062042 deberta\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--timestamp\",\n",
        "        action=\"store\",\n",
        "        dest=\"timestamp\",\n",
        "        help=\"Timestamp for versioning\",\n",
        "        default=default_checkpoint,\n",
        "        type=str,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--seed\",\n",
        "        action=\"store\",\n",
        "        dest=\"seed\",\n",
        "        help=\"Seed used for splits\",\n",
        "        default=48,\n",
        "        type=int,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--gpu\",\n",
        "        action=\"store\",\n",
        "        dest=\"gpu\",\n",
        "        help=\"GPU index to use\",\n",
        "        default=\"0\",\n",
        "        type=str,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "    args.seed = 201  #48  squad2, 159 distil , 201 roberta base , 99 deberta\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
        "    predictions = make_oofs(args.timestamp, args.seed, device=\"cuda\")"
      ],
      "id": "2fWhm3tiX_MI",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PosixPath('output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_0/epoch=02-rmse=0.5231.ckpt'), PosixPath('output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_1/epoch=01-rmse=0.4787.ckpt'), PosixPath('output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_2/epoch=04-rmse=0.5199.ckpt'), PosixPath('output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_3/epoch=01-rmse=0.5196.ckpt'), PosixPath('output/commonlitreadabilityprize/20211205-134729/distilroberta-base/fold_4/epoch=01-rmse=0.5101.ckpt')]\n",
            "5 models found. 5 tokenizers found. 5 configs found\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "OOF RMSE 0.37384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JquxL46KAKgd"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1g0pINURKhpWecDq5y9YbXacKGa2YRGeo"
      ],
      "id": "JquxL46KAKgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyYa7-zQcXGl"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1GEOnlnYQ-XOhPg8gJflAosfO9jNTF6fS"
      ],
      "id": "dyYa7-zQcXGl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5FOGLFmcXPX"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1dmzJVtD1gbvXD2s1jdwtjhFF6xeHZoqh"
      ],
      "id": "j5FOGLFmcXPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C8K4Gsyd6DH"
      },
      "source": [
        "mv /content/CommonLit_Readability_Prize/commonlitreadabilityprize/* output/commonlitreadabilityprize/"
      ],
      "id": "3C8K4Gsyd6DH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa8Vo1j3ggVP"
      },
      "source": [
        "rm -r output/commonlitreadabilityprize/*"
      ],
      "id": "Aa8Vo1j3ggVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeYt2V_EgLJa"
      },
      "source": [
        "!cp -R \"/content/output/commonlitreadabilityprize/squad2_oofs_0.49098.csv\" \"/content/drive/MyDrive/CommonLit Readability Prize\""
      ],
      "id": "jeYt2V_EgLJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo_bDuqcfRUE"
      },
      "source": [
        "!unzip output_deberta-base.zip"
      ],
      "id": "jo_bDuqcfRUE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kUr-4OqDe3D"
      },
      "source": [
        "!unrar x distilroberta.rar"
      ],
      "id": "0kUr-4OqDe3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj6vm-xJEEpj"
      },
      "source": [
        "#### ƒê√°nh gi√° k·∫øt qu·∫£"
      ],
      "id": "bj6vm-xJEEpj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9oMcP5g6vS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7fab90-07ca-4953-9fc7-99ff255fdd2b"
      },
      "source": [
        "df = pd.read_csv(\"output/commonlitreadabilityprize/20211204-023300/oofs_0.49098.csv\")\n",
        "df.shape"
      ],
      "id": "y9oMcP5g6vS6",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2834, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z1zo33mhqot",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "e658ad8b-eb63-4bbc-ba52-b55792b6274d"
      },
      "source": [
        "df.head()"
      ],
      "id": "7Z1zo33mhqot",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url_legal</th>\n",
              "      <th>license</th>\n",
              "      <th>excerpt</th>\n",
              "      <th>target</th>\n",
              "      <th>standard_error</th>\n",
              "      <th>fold</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6fceedb16</td>\n",
              "      <td>https://www.digitallibrary.io/en/books/details...</td>\n",
              "      <td>CC BY 4.0</td>\n",
              "      <td>It's carnival day. Hooray! shouts Little Mouse...</td>\n",
              "      <td>0.478452</td>\n",
              "      <td>0.553224</td>\n",
              "      <td>2</td>\n",
              "      <td>0.438988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6a548181f</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M. Leclerc du Sablon has published some of his...</td>\n",
              "      <td>-2.742465</td>\n",
              "      <td>0.503407</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.478664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3c0447a11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The cowherds loosened the buffalo's halter and...</td>\n",
              "      <td>-1.835394</td>\n",
              "      <td>0.505738</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.339717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5cb5ab998</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When they drew near Nottingham, all the people...</td>\n",
              "      <td>-1.541347</td>\n",
              "      <td>0.478166</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.952375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a83725b19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I know that many people are worrying about Sta...</td>\n",
              "      <td>-1.191174</td>\n",
              "      <td>0.475461</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.904002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ... prediction\n",
              "0  6fceedb16  ...   0.438988\n",
              "1  6a548181f  ...  -2.478664\n",
              "2  3c0447a11  ...  -0.339717\n",
              "3  5cb5ab998  ...  -0.952375\n",
              "4  a83725b19  ...  -0.904002\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VpxdBhzj1Ql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "d809566f-7ec3-47c5-d262-b14b729c53d3"
      },
      "source": [
        "n_bins = 20\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True, tight_layout=True)\n",
        "\n",
        "axs[0].hist(df.target, bins=n_bins);\n",
        "axs[1].hist(df.prediction, bins=n_bins);"
      ],
      "id": "-VpxdBhzj1Ql",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_results = Path(\"output/commonlitreadabilityprize\")\n",
        "kq = dict()\n",
        "for path in path_results.rglob(f\"*.csv\"):\n",
        "    li = str(path).replace(\".csv\",\"\").split(\"_\")\n",
        "    kq[li[0].split(\"/\")[-1]] = float(li[-1])\n",
        "\n",
        "plt.figure(figsize = (10, 6), dpi = 80)\n",
        "plt.barh(list(kq.keys()), list(kq.values()), align='center')\n",
        "\n",
        "for index, value in enumerate(kq.values()):\n",
        "    plt.text(value, index, str(value))\n",
        "plt.title(\"RMSE OOF\");\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "50GhLFOe6dQv",
        "outputId": "a907454e-22b0-45f0-fbfe-b36bf8d34904"
      },
      "id": "50GhLFOe6dQv",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAGcCAYAAADQycjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df5hV9X3g8fdHJiImDJgASnYYJ/IrukYRwoYiadVskywaGqHaWDGLiYC/alLXiKkxldRaN9mN2dXHgKkPWrFpuhJZq6lbTaMPJmpFRSMaEZthGB0EdRWyYgT57B/3Mh0GRmZwmIH5vl/Pcx/nnvM9537PHAfeHM69RGYiSZIkleCA3p6AJEmS1FOMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JWkPRAR90fE2xHxm4jYGBErI2JuuzE3R0RGxF+1W35ARPxrdd1/bLN8ekQ8EhGvR8QbEfFsRFzVbn9bqq/Z9nHpbuZ6ekQ81Gb8LyLitF2MOywiFkZEc0Rsrv53QUQc2m5cVte3ncNfdvV7KEm9wfiVpD337cz8ADAYmA98PyJ+r92YZ4AvRcT72iz7LPBm20ER8TvAYuCvgCHVx2nAqnb7+1FmfqDd49sdTTAiLgf+GlgADK8+FgA3RcS8NuMOA/4FGAF8CvgAcBJQDzzSPoCBz7Wbw+UdzUGS9iXGryS9R5m5LTP/HngNmNhu9RPAGmB6m2XnAQvbjZsMrM7MpZm5NTO3ZObTmfk3ezqviKgHrgQuzsxbMnNT9fE3wH8B/iIiRlSHfwvYAnw+M5/LzHcycxVwKvAOlbiXpP2e8StJ71FE1ETEHwMfAp7dxZDvA+dWxx4O/B7QPmqXAUdVbzP4XER8uBum9lkqv87fuot1fwP0Az5TfX4K8MPMfLvtoMz8LfB31fWStN8zfiVpz10SEa8Db1GJyXmZefcuxv0dcGxEjAXmAH+fmW+0HZCZ/wJMAQYA/xNort5H/Ll2+zq9ek9w28enOpjfUOCVasDuoLrsFWBYm7EvdrCf5jbjtlvabg4f6WBbSdqnGL+StOf+W2YOBg4BbgY+HRE17Qdl5mYqV1//BPgylXtud5KZD2fmf87Mj1C5N/c+YElEjG4z7O8zc3C7x087mN8GYEhE9G+/orpsCLC+zdh/18F+6tqM2+7z7ebw6w62laR9ivErSe9RZm4CLgCOqP53VxZQudd3bWYu78Q+Xwa+AbwP+NgeTu3/AAmcuYt1M4FtwD9Vn98N/FFEHNh2UPX5H1XXS9J+z/iVpG5QvY3gW8AVETFoF+ufBU4AztjV9hHx+Yj4ckR8OCpqga8Dm4HdxnIHc1oD/AVwbUScFREDq4+ZwHeBKzOzqTr8z6nccrEkIsZUP45tNLAEOLC6XpL2e8avJHWfW4FXgXm7WpmZyzJzdQfbvkrlEyEeA34DrAY+Dny2TaBC5eps+8/5vb6jCWXmfCpvtrsAaKk+LgTmZOZfthn3EvAfgHXA/VQ+iu2B6vj/kJnrdnfwkrQ/iMzs7TlIkiRJPcIrv5IkSSqG8StJkqRiGL+SJEkqhvErSZKkYhi/kiRJKsZO/xJRX9e/f/8cOnRob09DkiRJHXjxxRffzsyd/nXK7lBc/A4dOpTm5ubenoYkSZI6EBEb9ta+ve1BkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVIya3p5AT1v3xls0XHZ3b09DkiSpWzRec3JvT2G/4pVfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSeoDnn/+eSZPnsyYMWOYOHEiK1eu7HBsZnLSSScxePDgHZZ/5zvf4eijj+aoo47i1FNP5fXXX29d98gjj3DssccyZswYTjrpJF588cXWdbfeeivHHnssRx99NJ/61KdoampqXfeTn/yE8ePHM27cOI4++mhuueWWbjzqrjN+JUmS+oC5c+cyZ84cVq1axbx585g1a1aHY6+99lpGjhy5w7J7772XRYsW8dBDD/HMM88wYcIELr/8cgC2bdvGmWeeyfe+9z1WrVrF1KlT+epXvwrAr371K772ta9xzz338PTTT3P22Wdz3nnnAZXInjlzJjfffDMrVqzgrrvuYu7cuWzatGnvfBM6wfiVJEnaz61fv57ly5czc+ZMAGbMmMHatWtZvXr1TmNXrlzJ0qVLueyyy3ZY/uSTTzJlyhQGDhwIwNSpU7n11lsBeOyxx6ipqeHEE08EKqH9D//wD7z11ls8/fTTHHPMMQwfPrx1u3/8x3/k1VdfBSAiWq8gb9y4kQ996EP0799/L3wXOsf4lSRJ2s+tXbuW4cOHU1NTA1SCs76+fofbDwC2bNnC7NmzWbhwIf369dth3YQJE7jvvvtYt24dmcltt93Gpk2beO2112hqauLwww9vHTtw4EBqa2t56aWXOPbYY3n88cdZtWoVAIsXLyYzWbNmDRHBj370I6ZPn87hhx/OlClTuOWWWzjwwAP38nekY/tF/EbEByIiq18fFBFLI2JVRDwZEfdGxKjenqMkSdK+bv78+UyfPp0jjzxyp3Unnngil1xyCaeccgqTJk1i6NChAK1B3ZHRo0ezYMECvvjFL/Lxj3+cV199lcGDB1NTU8PWrVu56qqr+PGPf8yaNWv46U9/yllnncUrr7yyV46vM/aL+N2FG4GxmXks8L+Bv+7l+UiSJPWaESNG0NLSwtatW4HKvbZNTU3U19fvMO6BBx7guuuuo6GhgSlTprBx40YaGhrYsGEDAOeffz7Lly/nkUce4YQTTqCuro7a2lrq6+tZs2ZN6342bdrEG2+8wYc//GEA/vAP/5CHH36Y5cuXc95557F582ZGjRrFihUreOmll/jd3/1dACZOnEhdXR1PPPFET3xbdqlL8RsRAyLiRxHxTPWq6z9Vl18ZEc9HxGMRcVVENFaXN0TE6222b72CW31+W0Qsj4inIuLuiDiszbq51X0+Afzp9uWZ+VZm/iQzt+/nYaBhD45dkiSpTxg2bBjjx49n8eLFACxZsoS6ujpGjdrxL8eXLVvGmjVraGxs5MEHH6S2tpbGxsbWq7wtLS0AvPnmm3zzm9/k0ksvBSq3RGzZsoWf/exnACxcuJDPfe5zHHTQQTts98477zBv3jwuuOACDj744NYof/bZZwFYvXo1L7zwAmPHjt3L35GOvft17J19FhicmUcBRMQHI+Jk4DRgArAJuLUL+/tqZm6o7usy4Erg3Ig4GpgPHJeZLRFx9bvs4ytUrv7uUkRcDFzc+rz/wV2YniRJ0v5h4cKFzJo1i6uvvpra2loWLVoEwDnnnMO0adOYNm3abvfx6U9/mm3btvH2229z1llnceGFFwJwwAEHsHjxYubOnctbb73Fhz/84dY3wwF86UtfYs2aNfz2t7/l5JNP5uqrK+l26KGHcuONN3L66adzwAEHsG3bNq6//vqdrkj3pPi3C6idGBxxBHA/cBfwAPATKpH6RmbOr475JHBrZjZERAOwIjMHV9d9ANiUmVF9/hXgLOCg6uOVzJwUERdRCd+zq+NGAE3bt2sznz8DPgd8KjPf7Mwx1AwcknUX9O7ny0mSJHWXxmtO7u0pdLuIeDEz6/bGvrt020Nm/itwFHAPcDzwNHBI+2Ftvt4KtH0r4UHbv4iIKcBFwNTMPJrK1dmD2LWdCj0iLgGmA/+ps+ErSZKksnX1nt86IDPzTuASIIAngNMiYmBEBDCnzSbrKpvFUdXnX2yz7hAqt0m8GhEHAnPbrPtn4LNt7gE+t908LgbOAH4/M19HkiRJ6oSuftrDx4CfR8STVKL31sz8n8DtwOPAcqD1A+UycyvwJ8BdEfEo8L42+7oHeK76WAasaLPd01Tu/11WfcPbb7evqwb4fwcGAz+LiBUR8UgXj0OSJEkF6tI9v53aYeXNandlZkO37ribeM+vJEnqS7znt2v218/5lSRJkrqsqx91tlvVWxYaunu/kiRJ0nvllV9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFSMys7fn0KPq6uqyubm5t6chSZKkDkTEi5lZtzf27ZVfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxanp7Aj1t3Rtv0XDZ3b09DUmSpH1e4zUn9/YUup1XfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJ7+r5559n8uTJjBkzhokTJ7Jy5cqdxtx///0MGDCAcePGtT42b94MQGNjIyeccAKDBg1i3LhxO2170003MXr0aEaOHMns2bNbl0fECRGxOSJWtHkMqK47u93yVyLix7s7FuNXkiRJ72ru3LnMmTOHVatWMW/ePGbNmrXLcWPHjmXFihWtjwEDBgBQW1vLVVddxd/+7d/utM2vf/1rrrjiCpYtW8bq1at5+eWXAd7fZshzmTmuzWMzQGYuarscWAfctrtjMX4lSZLUofXr17N8+XJmzpwJwIwZM1i7di2rV6/u9D4++MEPMmXKFN7//vfvtO72229n2rRpHHbYYUQE5557LsDBXZljRHwCGAbcubuxxq8kSZI6tHbtWoYPH05NTQ0AEUF9fT1NTU07jX3hhRcYP348EydO5IYbbujU/puamjj88MNbnzc0NAD0azNkZEQ8HhGPRsT5Hezmy8Ctmblld69X06lZ7SUR8QrwceAG4E8z87l3GXslcE1mvlV9/i0ql8Fvq64bnJlf3fuzliRJUnvjx4+nubmZQYMG0dzczNSpUxkyZAinn376e9nt40BdZr4REXXATyLilcz8++0DIuL9wBeASZ3Z4T5x5Tczp75b+Fb9OXBQm22+mZm7va9DkiRJe27EiBG0tLSwdetWADKTpqYm6uvrdxhXW1vLoEGDAKirq+OMM85g2bJlu91/fX09a9asaX3e2NgI8E71tTZm5hvVr5uBHwKfbLeL04CVmflMZ46nR+M3IqZFxLMR8VREfLvN8saIGFf9+hvVMdvfuXd4RCyoDl1WXTYsIm6OCK/0SpIk7UXDhg1j/PjxLF68GIAlS5ZQV1fHqFGjdhjX0tLCtm3bANi0aRN33XUXxx133G73P2PGDO68807WrVtHZrJgwQKANwEiYnhEHFD9eiBwCvBEu118Gbips8fTY/EbEcOARcCMzDwGWA18qN2YQ4BLgPHVd+1NBl7OzHOrQz5ZfUff+i687sUR0bz9sW3L5m45HkmSpFIsXLiQhQsXMmbMGK655hoWLVoEwDnnnMOdd1beY7ZkyRI+9rGPceyxxzJp0iR+//d/n7PPPhuAN998k7q6Ok477TSeeeYZ6urq+PrXvw7AEUccwfz58zn++OMZNWoUQ4cOBfh/1ZeeAfwyIp4EHgbupdKTAETEWGAc8KPOHktk5nv6ZnT6hSKmUbmv98Tq835Uqn4scD/weeCXVA6sCfgn4O7qJW4iIoFDMvP16vObgRWZ+b2u3PNbM3BI1l1wS/cenCRJUh/UeM3JvfK6EfFiZtbtjX335j2/O1V3Zr5D5Wbl71H5uIqHI6L9fR2SJEnSHunJ+H0IOCYiPlp9/iXgwLYDqvdyHJqZyzLzL4AHge03i2wCBvXUZCVJktT39NhHnWXmhoj4EnBHRLwN3AO82m7YIOD26kdWJPA8sP0ehf8O3BsRbwKf7qFpS5IkqQ/psXt+9xXe8ytJktQ53vMrSZIk7ceMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVIzKzt+fQo+rq6rK5ubm3pyFJkqQORMSLmVm3N/btlV9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBWjprcn0NPWvfEWDZfd3dvTkCRJ6laN15zc21PYL3jlV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJEnFMH4lSZJUDONXkiRJxTB+JUmSVAzjV5IkScUwfiVJklQM41eSJKkPef7555k8eTJjxoxh4sSJrFy5ssOxmclJJ53E4MGDd1j+ne98h6OPPpqjjjqKU089lddffx2Al156ic985jOMHTuWY445hhkzZrBhw4ad9rto0SIigqVLl3bvwXUD41eSJKkPmTt3LnPmzGHVqlXMmzePWbNmdTj22muvZeTIkTssu/fee1m0aBEPPfQQzzzzDBMmTODyyy8HoF+/flxxxRU899xzPPXUUxxxxBF87Wtf22H7xsZGfvCDHzBp0qRuP7buYPxKkiT1EevXr2f58uXMnDkTgBkzZrB27VpWr16909iVK1eydOlSLrvssh2WP/nkk0yZMoWBAwcCMHXqVG699VYADj30UKZMmdI69hOf+ASNjY2tz7dt28Y555zDddddR//+/bv78LqF8StJktRHrF27luHDh1NTUwNARFBfX09TU9MO47Zs2cLs2bNZuHAh/fr122HdhAkTuO+++1i3bh2ZyW233camTZt47bXXdhj3zjvvcP311/MHf/AHrcu++93vcvzxxzNhwoS9dITv3V6N34hojIhx3bCfcRHxhe6YkyRJUunmz5/P9OnTOfLII3dad+KJJ3LJJZdwyimnMGnSJIYOHQrQGtRQuVf4/PPP55BDDuErX/kKAE8//TRLlizhG9/4Rs8cxB6q2f2QjkVETWZu7a7JdPQawDjg88Df7c3XkiRJ2p+NGDGClpYWtm7dSk1NDZlJU1MT9fX1O4x74IEHaGpq4vrrr2fr1q1s3LiRhoYGHn30UYYOHcr555/P+eefD8DDDz9MXV0dtbW1rdtfdNFFrF27lqVLl3LAAZVrqcuWLaOxsZHRo0cDsG7dOubMmUNLSwvnnXdeD30Hdq/LV34jIiNifkQ8CvxVRAyLiB9HxC8j4umImNtukzMj4rGIWB0RX2uzn9ERcXdEPBoRT0XEhR28xg+AbwEnRsSKiFhQHXNbRCyvbnt3RBy2R98BSZKkPmLYsGGMHz+exYsXA7BkyRLq6uoYNWrUDuOWLVvGmjVraGxs5MEHH6S2tpbGxsbWq7wtLS0AvPnmm3zzm9/k0ksvbd32oosuYvXq1dxxxx0ceOCBrcvPO+88WlpaaGxspLGxkUmTJnHjjTfuU+ELe37l953MnAgQET8CnsvM6RExDHgsIp7MzIerYw8FPg58CHg8In4OPAL8EJiZmb+KiIOBhyPikcx8dBevMQv4fGZ+vs0cvpqZG6rrLwOuBM5tP9GIuBi4uPV5/4P38JAlSZL2fQsXLmTWrFlcffXV1NbWsmjRIgDOOeccpk2bxrRp03a7j09/+tNs27aNt99+m7POOosLL6xco/z5z3/Oddddx0c/+lE+8YlPAPCRj3yEO+64Y+8dUDeLzOzaBhEJjMjM5urzV4EJmdlYff4/gA2ZeVVENAL/OTMfqK77HvAacDvwGPBcm10PAq7MzFt28RqzaBe/EfEV4CzgoOrjlczc7Wdq1AwcknUX3NKlY5YkSdrXNV5zcm9PodtExIuZWbc39r2nV35/8y7rdlfTCQTwWma+25vhOnyNiJgCXAT8Tmauj4hpVG6NkCRJkjrUHZ/2cB8wGyAihgLTgXvbrJ9VXfdB4FTgp1Su+G6MiLO3D4qIUdUxu7KRypXh7Q4BNgGvRsSBQPv7jCVJkqSddEf8XgQcGRG/BH4G/GVmPtJm/YaIeAz4F+D6zPxF9RMiTgGmV9+wthK4CRjQwWv8FOhfHbsAuIdKQD8HLANWdMNxSJIkqY/r8m0PmRntnr9M5WrvrsY2vMt+XgA+18nXeAOY3G7YH7V7fnlHryVJkiSB/8KbJEmSCmL8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKEZnZ23PoUXV1ddnc3Nzb05AkSVIHIuLFzKzbG/v2yq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRg1vT2BnrbujbdouOzu3p6GJEnSPqHxmpN7ewo9yiu/kiRJKobxK0mSpGIYv5IkSSqG8StJkqRiGL+SJEkqhvErSZKkYhi/kiRJKobxK0mSpGIYv5IkSSqG8StJkqRiGL+SJEkqhvErSZKkYhi/kiRJKobxK0mSpGIYv5IkSSqG8StJkqRiGL+SJEkqhvErSZKkYhi/kiRJKobxK0mSpGIYv5IkSSqG8StJkqRiGL+SJEkqhvErSZKkYhi/kiRJKobxK0mSpGIYv5IkSSqG8StJklS4559/nsmTJzNmzBgmTpzIypUrdxpz//33M2DAAMaNG9f62Lx5c+v6m266idGjRzNy5Ehmz57Nli1bdtg+MznppJMYPHhw67Lf/OY3fOYzn2HIkCE7LG8vIm6OiIyIjgd1kvErSZJUuLlz5zJnzhxWrVrFvHnzmDVr1i7HjR07lhUrVrQ+BgwYAMCvf/1rrrjiCpYtW8bq1at5+eWXufHGG3fY9tprr2XkyJE7LHvf+97HvHnzuO+++zqcW0RMB7Z0OKCLjF9JkqSCrV+/nuXLlzNz5kwAZsyYwdq1a1m9enWn93H77bczbdo0DjvsMCKCc889lx/+8Iet61euXMnSpUu57LLLdtiuf//+O10NbisiDgX+DLi4ywfWAeNXkiSpYGvXrmX48OHU1NQAEBHU19fT1NS009gXXniB8ePHM3HiRG644YbW5U1NTRx++OGtzxsaGlq337JlC7Nnz2bhwoX069evq9P7AXBpZm7q6oYdqdnTDSPiFeDjmdn4LmMSOCQzX9/T16nu5wTgoMy8573sR5IkSXtm/PjxNDc3M2jQIJqbm5k6dSpDhgzh9NNPf9ft5s+fz/Tp0znyyCNpbGzs9OtFxDlAU2b+83ub+Y72+Su/EVEDnAB8tpenIkmS1OeMGDGClpYWtm7dClTemNbU1ER9ff0O42praxk0aBAAdXV1nHHGGSxbtgyA+vp61qxZ0zq2sbGxdfsHHniA6667joaGBqZMmcLGjRtpaGhgw4YNu5vaicAfRERjRDRWlz0VEce9l+PtdPxGxLSIeDYinoqIb7dZPjoi7o6IR6vrLmy36SUR8URErIqIM9tsNzEi/jkillfXn1Zd3hARr0fEf42Ix4ELgXOBMyNiRUR8MyJqIuL/VLddGRF/GxHvfy/fCEmSpBINGzaM8ePHs3jxYgCWLFlCXV0do0aN2mFcS0sL27ZtA2DTpk3cddddHHdcpUNnzJjBnXfeybp168hMFixYwBe+8AUAli1bxpo1a2hsbOTBBx+ktraWxsZGhg4d+q7zyswzM3NEZjZkZkN18TGZ+cR7Od5O3fYQEcOARcAnM/OZiJgDfAjoB/wQmJmZv4qIg4GHI+KRzHz03+aex0XEEcDyiPg58DpwIzA1M1siYgjweET8orrNIGBlZs6rvv5gYHBmfrX6PIA/zsxXq1/fAPwJcM0u5n4xbW6Sjv4Hd+HbI0mS1PctXLiQWbNmcfXVV1NbW8uiRYsAOOecc5g2bRrTpk1jyZIlfP/736empoatW7dy2mmncfbZZwNwxBFHMH/+fI4//ngATjjhBObOndup1z7mmGPYsGEDGzdupK6ujhNPPHHvHGRVZObuB0VMA/40M0+sPu8HvAkcBzwGPNdm+CDgysy8pXrPb0NmrqlutxT4MfAKlWj+dd+a0q4AAAa+SURBVJvtPgjMAv4VWEXlHt9t1e2uZMf4PQD4FnAylYAfBPwiM7+wu2OpGTgk6y64ZbfHLEmSVILGa07u7SnsJCJezMy6vbHvPX3D2/ZiDuC1zBzXxW2DypXdye1XRkQD8Ob28O3AHwMnAb+XmRsj4qLqc0mSJKlDnb3n9yHgmIj4aPX5l4ADgd8CGyPi7O0DI2JURHywzbZnV5c3AJ8ElgG/AD4SEf+xzXbjIuLADl5/I5Wru9sdArxSDd+BVK4YS5IkSe+qU/GbmRuoBO8dEfEkMBp4FdgKnAJMr77ZbSVwEzCgzeb9IuIJ4J+AizKzMTP/L5VbFv4sIp6MiGeo3K/b0XzuAMZtf8Mb8DfAwRHxHPCPVIJakiRJeleduue3L/GeX0mSpH9T2j2/+/zn/EqSJEndxfiVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBUjMrO359Cj6urqsrm5ubenIUmSpA5ExIuZWbc39u2VX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVw/iVJElSMYxfSZIkFcP4lSRJUjGMX0mSJBXD+JUkSVIxjF9JkiQVIzKzt+fQoyJiK7Cut+ehbvEB4De9PQl1G89n3+G57Fs8n33L/nI+h2Zm/72x45q9sdN93LrMrOvtSei9i4hmz2Xf4fnsOzyXfYvns2/xfHrbgyRJkgpi/EqSJKkYJcbvd3t7Auo2nsu+xfPZd3gu+xbPZ99S/Pks7g1vkiRJKleJV34lSZJUKONXkiRJxTB+JUmSVIw+F78RMToifhERqyLi0Yj49x2M+3JEPB8RL0TEDyLifT09V+1eZ85nRDRExP0R8UZErOiNeapzOnk+T4qIf4mIZyJiZUR8OyL63K9V+7tOnsvfiYgV1cfKiFgYEXvlQ+v13nT2987q2IiIf46I13tyjuq8Tv58nhARm9v8jK6IiAG9Md+e1hd/Q1kI3JiZY4D/CtzcfkBEfAT4C+CTwCjgUGBOD85Rnbfb8wlsBL4B/HEPzkt7pjPn8/8CX8jMo4AJwGTgiz02Q3VWZ87lk8DEzBwHfAwYBpzfYzNUV3TmfG73p8ALPTEp7bHOns/nMnNcm8fmHpthL+pT8RsRw4CPA4uri5YAIyJiVLuhfwjcmZnrsvJxFwuAM3pupuqMzp7PzHwtMx8E/l8PT1Fd0IXz+URm/mv167eAFUBDD05Vu9GFc/lmZm6pPj0QGAD4EUP7mC783kn1CuLngWt6bobqiq6cz1L1qfgFRgAtmbkVoBq2TUB9u3H1wJo2zxt3MUa9r7PnU/uHLp/PiDiMyh9W7+qRGaqzOn0uq7clPQm8ArwB3NCTE1WndOp8Vm8P/AEwF3inpyepTuvKr7UjI+Lx6q0RxfytTF+LX0l9RETUAv8AfDszl/f2fLRnMrMxM48FDgP6A9N7eUrac38O/Dgzn+3tiahbPA7UZeZ44FTg3Ig4vZfn1CP6WvyuBYZHRA1Ubsqn8iedpnbjmoDD2zxv2MUY9b7Onk/tHzp9PiNiIHAP8L8zs/h/jWgf1OWfzcz8DfB3wJk9MkN1RWfP5+8BfxIRjcCDQG1ENEbE0J6crHarU+czMzdm5hvVr5uBH1J5L1Sf16fiNzPXU/mTzMzqohlAc2aubjd0CTAtIg6r/k9xLpVflLUP6cL51H6gs+czIj5AJXzvycyrenaW6owunMtR2z9JJyIOpHJ16amenKt2r7PnMzM/mZmHZ2YDMAXYmJkNmbmhRyesd9WFn8/h2z9Jp3rB4RTgiZ6ca2/pc/+8cUSMpfKuxg9R+RSAszPzlxHx11Te5HZnddxs4LLqZvcD57Z5Y4b2EZ05nxFxMLCKyl+pDgLWA7dm5td7adrqQCfP5+XAlcDKNpv+r8z8y56erzrWyXM5B7iIyv2hNcBPgUurb2TUPqSzv3e2Gd8ArMjMwT08VXVCJ38+LwTOA7ZS+fn8X8D87GthuAt9Ln4lSZKkjvSp2x4kSZKkd2P8SpIkqRjGryRJkoph/EqSJKkYxq8kSZKKYfxKkiSpGMavJEmSimH8SpIkqRj/H6gpOBXr/T2NAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 800x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXT3qEFBrcZ0"
      },
      "source": [
        "#### Submission "
      ],
      "id": "OXT3qEFBrcZ0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDlRFwKgrg0y"
      },
      "source": [
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "import gc\n",
        "\n",
        "\n",
        "def infer(model, dataset, batch_size=32, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for input_dict, _, features in loader:\n",
        "            input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
        "            mean, log_var = model(features.to(device), **input_dict)\n",
        "            predictions.append(mean.cpu())\n",
        "\n",
        "    return torch.cat(predictions, 0)\n",
        "\n",
        "def netflix(es, ps, e0, l=0.0001):\n",
        "    \"\"\"Combine predictions with the optimal weights to minimize RMSE.\n",
        "\n",
        "    Ref: T√∂scher, A., Jahrer, M., & Bell, R. M. (2009). The bigchaos solution to the netflix grand prize.\n",
        "\n",
        "    Args:\n",
        "        es (list of float): RMSEs of predictions\n",
        "        ps (list of np.array): predictions\n",
        "        e0 (float): RMSE of all zero prediction\n",
        "        l (float): lambda as in the ridge regression\n",
        "\n",
        "    Returns:\n",
        "        (tuple):\n",
        "\n",
        "            - (np.array): ensemble predictions\n",
        "            - (np.array): weights for input predictions\n",
        "    \"\"\"\n",
        "    m = len(es)\n",
        "    n = len(ps[0])\n",
        "\n",
        "    X = np.stack(ps).T\n",
        "    pTy = 0.5 * (n * e0 ** 2 + (X ** 2).sum(axis=0) - n * np.array(es) ** 2)\n",
        "\n",
        "    w = np.linalg.pinv(X.T.dot(X) + l * n * np.eye(m)).dot(pTy)\n",
        "\n",
        "    return X.dot(w), w"
      ],
      "id": "qDlRFwKgrg0y",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHi9dUFfxTbg"
      },
      "source": [
        "rm -r /content/distilroberta"
      ],
      "id": "fHi9dUFfxTbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ovQ5LH4xKtX"
      },
      "source": [
        "mv /content/distilroberta/20211205-134729 /content/output/commonlitreadabilityprize/"
      ],
      "id": "3ovQ5LH4xKtX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8P3xadrvzo-"
      },
      "source": [
        "rm -r /content/output/commonlitreadabilityprize/20211204-110832"
      ],
      "id": "x8P3xadrvzo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HUypocesR2o"
      },
      "source": [
        "mv /content/distilroberta/20211206-050156/distilroberta-base/fold_4 /content/distilroberta/20211205-134729/distilroberta-base/"
      ],
      "id": "4HUypocesR2o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_CQb_l2zQz6"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/CommonLit Readability Prize/\"*.csv ."
      ],
      "id": "2_CQb_l2zQz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRfeSggkzkG_"
      },
      "source": [
        "!mv /content/roberta_base_oofs_0.49240.csv /content/output/commonlitreadabilityprize/20211206-015956/"
      ],
      "id": "SRfeSggkzkG_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsk1JwvSsGw_"
      },
      "source": [
        "def make_predictions(dataset_paths, device=\"cuda\"):\n",
        "    mpaths, oof_paths = [], []\n",
        "    for p in dataset_paths:\n",
        "        mpaths.append(sorted(list(p.rglob(f\"*.ckpt\"))))\n",
        "        oof_paths.extend(sorted(list(p.glob(f\"*.csv\"))))\n",
        "\n",
        "    print(\n",
        "        f\"{len([item for sublist in mpaths for item in sublist])} models found.\",\n",
        "        f\"{len(oof_paths)} OOFs found\",\n",
        "    )\n",
        "    print(mpaths)\n",
        "    # Construct OOF df\n",
        "    oofs = pd.read_csv(INPUT_PATH / \"train.csv\", usecols=[\"id\", \"target\"]).sort_values(\n",
        "        by=\"id\"\n",
        "    )\n",
        "    for i, p in enumerate(oof_paths):\n",
        "        x = pd.read_csv(p).sort_values(by=\"id\")\n",
        "        oofs[f\"model_{i}\"] = x[\"prediction\"].values\n",
        "\n",
        "    df = pd.read_csv(INPUT_PATH / \"test.csv\")\n",
        "    output = 0\n",
        "\n",
        "    for i, group in enumerate(mpaths):\n",
        "        output = 0\n",
        "        for j, p in enumerate(group):\n",
        "            ckpt_size = p.stat().st_size / 1024 ** 3\n",
        "            bs = 32 if ckpt_size > 1.5 else 64\n",
        "            print(f\"{i} {p}, Size: {ckpt_size:0.2f}\")\n",
        "\n",
        "            if j == 0:\n",
        "                config = AutoConfig.from_pretrained(str(p.parent))\n",
        "                tokenizer = AutoTokenizer.from_pretrained(str(p.parent))\n",
        "                dataset = CommonLitDataset(df, tokenizer)\n",
        "                print(p.parent)\n",
        "            \n",
        "            print(p)\n",
        "            model = CommonLitModel.load_from_checkpoint(p, hf_config=config)\n",
        "            output += infer(model, dataset, batch_size=bs, device=device)\n",
        "\n",
        "            del model\n",
        "\n",
        "        del dataset\n",
        "        del tokenizer\n",
        "        gc.collect()\n",
        "\n",
        "        df[f\"model_{i}\"] = output.squeeze().numpy() / len(group)\n",
        "\n",
        "    pred_cols = [f\"model_{i}\" for i in range(len(mpaths))]\n",
        "\n",
        "    # Use mean\n",
        "    # df[\"target\"] = df[pred_cols].mean(1)\n",
        "\n",
        "    # Stack using linear regression\n",
        "#     print(oofs[pred_cols].corr())\n",
        "    reg = RidgeCV(alphas=(0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 50, 100, 500, 1000), normalize=True)\n",
        "#     reg = LassoCV(max_iter=5000, random_state=48, n_jobs=-1, normalize=True)\n",
        "    reg.fit(oofs[pred_cols], oofs[\"target\"])\n",
        "#     print(f\"Weights: {reg.coef_}, bias: {reg.intercept_}\")\n",
        "    print(f\"Best RMSE: {np.sqrt(-reg.best_score_):0.5f}. Alpha {reg.alpha_}\")  # Ridge\n",
        "#     print(f\"Best RMSE: {np.sqrt(reg.mse_path_)[-1].mean():0.5f}\")  # Lasso\n",
        "    df[\"target\"] = reg.predict(df[pred_cols])\n",
        "    \n",
        "    # Stack using Netflix method\n",
        "#     oof_preds = [oofs[c].values for c in pred_cols]\n",
        "#     rmses = [np.sqrt(mean_squared_error(p, oofs[\"target\"])) for p in oof_preds]\n",
        "#     ensemble, weights = netflix(rmses, oof_preds, 1.4100)\n",
        "#     score = np.sqrt(mean_squared_error(ensemble, oofs[\"target\"]))\n",
        "#     print(f\"Best RMSE: {score:0.5f}\")\n",
        "#     df[\"target\"] = df[pred_cols] @ weights\n",
        "\n",
        "    # Netflix with 3-seed CV\n",
        "    #     X, y = oofs[pred_cols], oofs[\"target\"]\n",
        "    #     scores = []\n",
        "    #     weights_agg = 0\n",
        "    #     for seed in [48, 42, 3]:\n",
        "    #         for fold, (trn_idx, val_idx) in enumerate(create_folds(X, y, random_state=seed)):\n",
        "    #             train_oofs = X.loc[trn_idx]\n",
        "    #             valid_oofs = X.loc[val_idx]\n",
        "    #             valid_target = y.loc[val_idx]\n",
        "\n",
        "    #             train_preds = [train_oofs[c].values for c in X.columns]\n",
        "    #             rmses = [np.sqrt(mean_squared_error(X[c], y)) for c in X.columns]\n",
        "    #             _, weights = netflix(rmses, train_preds, 1.4100)\n",
        "\n",
        "    #             val_pred = valid_oofs @ weights\n",
        "    #             score = np.sqrt(mean_squared_error(val_pred, valid_target))\n",
        "    #             scores.append(score)\n",
        "    #             weights_agg += weights\n",
        "\n",
        "    #     weights_agg /= len(scores)\n",
        "    #     print(f\"CV RMSE: {np.mean(scores):0.5f}\")\n",
        "    #     df[\"target\"] = df[pred_cols] @ weights_agg\n",
        "\n",
        "    df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model_folders = [\n",
        "        \"20211204-023300\",\n",
        "        \"20211205-062042\",\n",
        "        \"20211205-134729\",\n",
        "        \"20211206-015956\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    dataset_paths = [OUTPUT_PATH / f for f in model_folders]\n",
        "\n",
        "    predictions = make_predictions(dataset_paths, device=\"cuda\")"
      ],
      "id": "xsk1JwvSsGw_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpGuZbsiik1T"
      },
      "source": [
        "#### Notes:    \n",
        "- kl_loss (Kullback‚ÄìLeibler divergence) trong to√°n th·ªëng k√™ (information gain) √ù t∆∞·ªüng d·ª±a tr√™n kho·∫£ng c√°ch gi·ªØa 2 ph√¢n ph·ªëi (https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html).\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "mpGuZbsiik1T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Leaderboard (RMSE)\n",
        " - Top1 : 12 - 0.447 , 1 - 0.446\n",
        " - T√°c gi·∫£: 32 - 0.451 , 4 - 0.447 \n",
        " - Nh√≥m:  0.475 , 0.469 \n",
        "\n",
        "**Nh·∫≠n x√©t**\n",
        "\n",
        " - Kh√≥ ƒë·ªÉ ƒë·∫°t k·∫øt qu·∫£ t·ªët nh∆∞ t√°c gi·∫£ ( V√¨ gi·ªõi h·∫°n v·ªÅ t√†i nguy√™n kh√¥ng th·ªÉ hu·∫•n luy·ªán c√°c m√¥ h√¨nh c√≥ k√≠ch th∆∞·ªõc l·ªõn v√† th·ªùi gian hu·∫•n luy·ªán).\n",
        " - D·ª± ki·∫øn nh√≥m s·∫Ω t√¨m c√°ch c·∫£i thi·ªán k·∫øt qu·∫£ d·ª±a tr√™n c√°c kh√≠a c·∫°nh t·ª´ c√°c m√¥ h√¨nh trong kh·∫£ nƒÉng v√† ph∆∞∆°ng ph√°p kh√°c (turning, add properties , method ensemble ...)"
      ],
      "metadata": {
        "id": "g9ptaON40Mh7"
      },
      "id": "g9ptaON40Mh7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U0zOCln1ZOW"
      },
      "source": [
        "## Nh√¨n l·∫°i qu√° tr√¨nh l√†m ƒë·ªì √°n"
      ],
      "id": "8U0zOCln1ZOW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72079acf"
      },
      "source": [
        "Sau bao ng√†y v·∫•t v·∫£ l√†m ƒë·ªì √°n th√¨ b√¢y gi·ªù ƒë√£ k·∫øt th√∫c. B√¢y gi·ªù l√† l√∫c ƒë·ªÉ ng·ªìi u·ªëng coffee v√† tƒ©nh t√¢m nh√¨n l·∫°i qu√° tr√¨nh l√†m.\n",
        "\n",
        "- M·ªói th√†nh vi√™n: ƒê√£ g·∫∑p nh·ªØng kh√≥ khƒÉn g√¨? (Hay m·ªçi chuy·ªán ƒë·ªÅu thu·∫≠n l·ª£i)\n",
        "- M·ªói th√†nh vi√™n: C√≥ h·ªçc ƒë∆∞·ª£c g√¨ h·ªØu √≠ch? (Hay kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨)\n",
        "- Nh√≥m: N·∫øu c√≥ th√™m th·ªùi gian th√¨ s·∫Ω l√†m g√¨?\n",
        "\n",
        "Ph·∫ßn n√†y c√≥ sao th√¨ b·∫°n n√≥i v·∫≠y th√¥i, ch·ª© kh√¥ng ph·∫£i l√† vi·∫øt\n",
        "cho c√≥, ho·∫∑c t·ª± ch·∫ø ra ƒë·ªÉ nghe cho hay."
      ],
      "id": "72079acf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ae45e1e"
      },
      "source": [
        "## T√†i li·ªáu tham kh·∫£o"
      ],
      "id": "4ae45e1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9701b2bd"
      },
      "source": [
        "ƒê·ªÉ ho√†n th√†nh ƒë·ªì √°n n√†y, nh√≥m b·∫°n ƒë√£ tham kh·∫£o nh·ªØng t√†i li·ªáu n√†o?"
      ],
      "id": "9701b2bd"
    }
  ]
}